---
title: "Comparison of Pollen Traps"
subtitle: "Evaluation of Similarity and Robustness of Three Hirst-type Pollen Traps Located in Payerne During the Blooming Season 2013"
author: "Simon Adamov"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
always_allow_html: TRUE
output:
  html_document:
    df_print: paged 
  pdf_document: default
  word_document: default
---

# Setup

```{r}
##### PLEASE DEFINE SPECIES AND TEMPORAL RESOLTION HERE #####

# rmarkdown::render("vignettes/hirstcomparison.Rmd", output_file = "C:/Users/ads/Desktop/daily_total.html")

species <- "Total"
resolution <- "daily" # What temporal resolution should be plotted c("daily", "12hour", "6hour", "3hour", "2hour", "hourly")

```


```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      error = FALSE, 
                      warning = FALSE, 
                      message = FALSE,
                      fig.retina =3,
                      fig.width = 10,
                      fig.height = 7,
                      out.width = "100%",
                      out.height = "100%")
# This project is using renv dependency management, for more info: https://cran.r-project.org/web/packages/renv/vignettes/renv.html

library(caTools)
library(httpgd)
library(languageserver)
library(MASS)
library(tidyverse)
library(lubridate)
library(ggpubr)
library(here)
library(lme4)
library(psych)
library(robustlmm)
library(nparcomp)
library(goftest)
library(kableExtra)
library(AeRobiology)
library(ggthemr) # On Windows I load these functions directly from the R-folder of this project, because installing from gitlab is not possible

# library(Amelia) # only used for one fct call to missmap()
# library(data.table) # only used for transpose()
# library(dunn.test)
# library(conover.test)

devtools::load_all()

# I like the look of these plots
# devtools::install_github('cttobin/ggthemr')

ggthemr("fresh")



# caTools and bitops are required to knit this vignette, those are not tracked by renv and must be installed by the user
# Due to old R-Version caTools must be installed from CRAN Archive, in order to knit markdown documents.
# packageurl <- "http://cran.r-project.org/src/contrib/Archive/caTools/caTools_1.17.1.1.tar.gz"
# install.packages(packageurl, repos=NULL, type="source")

```
Bio-aerosols have important impacts on human health (EACCI, 2015; Pawankar et al., 2013) and agriculture (Cunha et al. 2016; Oteros et al. 2014), and serve as important indicators for identification of invasive species (Karrer et al., 2015; Sikoparija et al., 2009) and climate change (Zhang et al. 2015; Ziello et al. 2012). As such, networks to monitor these particles, particularly pollen and spores, have been established in many countries over the past decades (Hertel et al., 2013). At present, the majority of these networks employ manual methods based on Hirst-type volumetric samplers (Hirst, 1952) for monitoring purposes. Air sucked is drawn through these devices and particles impact onto a rotating tape, which is usually collected and analysed using light microscopy manually on a weekly basis.

Standards to ensure comparability within and across networks have been established for routine monitoring of airborne pollen and spores (Galan et al. 2014; CEN/TS 16868, 2015). Nevertheless, these methods are known to suffer from a number of shortcomings, including collection efficiency onto the rotating tape (Orlandi et al,. 2014; Mandrioli et al., 1998),  errors in estimates of the flow-rate used to calculate concentrations (Oteros et al., 2017), sampling efficiency (Sikoparija et al., 2011; Tormo-Molina et al., 1996; Käpylä and Penttinen 1981),  issues related to the manual counts (Rojo, 2019; Comptois et al., 1999; Pedersen and Moseholm, 1993), and quality control (Sikoparija et al., 2016; Berti et al. 2009). 

While several studies have compared measurements from separate devices of the same kind, they have rarely been relatively few comparisons of more than two devices and often these have been at considerable distance from one another (e.g. Irdi et al., 2002; Gottardini and Cristofolini, 1997; Ekebom et al., 1997; Rizzi et al., 1992; Meiffren, 1988).  Molina et al. 2013 compared five traps at the same site and found significant differences in terms of the timing of peak pollen concentrations when hourly data were considered, however, no such differences were evident in the analysis of daily data. In contrast, Rojo et al. 2019 compared three devices in Munich, Germany, and two traps in Zrenjanin, Serbia, and found average standard deviations of 34% between Hirst-type traps, even when daily averages were considered. Although ratios of mean pollen concentrations between the instruments were mostly close to 1, they found that the standard deviations of the pollen concentrations varied significantly on the daily scale; a possible explanation for why no significant differences in previous work (Rojo et al., 2019). 

Fully understanding all source of errors related to traditional manual measurements is particularly important given the advent of automatic pollen monitors based on new technologies such as image-recognition (Oteros et al., 2019?) and air-flow cytometry (Sauvageat et al., 2019; Crouzy et al., 2016). For validation purposes, these new, real-time instruments are compared against traditional, manual observations and it is thus essential to understand what level of error can be expected from the various measurement techniques to make robust comparisons. 

In this study we compare measurements from three Hirst-type traps placed side-by-side in Payerne, Switzerland, from the pollen season of 2013. While these observations date back somewhat, counting methods have not changed significantly since and the data remain valid. We aim to investigate more fully the variability between measurements at the same location at both the daily and hourly scale. 

# Data

Three Hirst-type traps (Burkard Manufacturing, UK) were co-located on the roof of the Federal Office of Meteorology and Climatology MeteoSwiss building in Payerne, Switzerland. The roof is flat and at a height of 7m. The Hirst-type traps were placed in a line, approximately 3m from each other. The study period ran from 26 March to 25 July 2013 and the three Hirst-type traps operated simultaneously for this entire period.

The three Hirst-type traps (numbered 2, 4, and 8) were carefully calibrated and set for seven-day sampling using Melinex tape coated with silicon (Lanzoni, Italy). The volume of air sampled was checked each week using a flow meter when the drums were exchanged. Tapes were cut into seven daily segments and mounted onto microscope slides. Two longitudinal scans were counted on each slide using an Olympus microscope with a 600x magnification and, at a later stage, two additional lines were counted at 400x magnification. An average of all four lines was taken to calculate the final concentrations. P Data are expressed as daily average concentrations (pollen grains/m3). 
All pollen types were counted and included in the “total pollen” category. We also selected 20 individual pollen types for a more precise comparison: Urtica, Castanea, Salix, Platanus, Betula, Alnus, Corylus, Fraxinus, Rumex, Taxus, Cupressus, Poaceae, Ulmus, Populus, Plantago, Quercus, Fagus Juglans Carpinus, Picea, and Pinus. This list aims to include the most important pollen types measured during the 2013 campaign in Payerne and to cover wide range of pollen sizes to provide a better understanding of how pollen counts vary with grain size. 

## Import

```{r cache = TRUE, cache.lazy = FALSE}

# The data was provided by Fiona Tummon, in form of 10-minute pollen counts per line and trap.
# The data prep until that point was not re-evaluated.
# The datetimes were provided in a seperate file (in Payerne local time)
# PAY2, PAY4 and PAY8 are three different pollen traps
# lines 2, 4, 6, 7 are four different lines, along which the pollen were manually counted under the microsope.
# Lines 2 and 6 were counted in 2013, whereas lines 4 and 7 were counted in 2019

# THE DATA IS HUGE AND ORIGINALLY STORED HERE: 
# M:\zue-prod\climate\others\ads\hirstcomparison\ext-data
# I suggest to copy the data into your local project repo

path_data <- paste0(here::here(), "/ext-data/")
files_data <- list.files(path_data, pattern = "2013_10min.csv")
files_datetimes <- list.files(path_data, pattern = "dates_10min.csv")

data_raw <-
  map(files_data, ~ read_delim(
    paste0(path_data, .x),
    delim = ";",
    col_names = TRUE,
    col_types = cols(.default = "n"),
    na = "nan"
    )
  ) %>% 
  setNames(files_data)

dates_raw <-
  map(files_datetimes,~ read_delim(
    paste0(path_data, .x),
    delim = ";",
    col_names = FALSE,
    col_types = cols(.default = "n") # dates and times are in a weird format, have to import as numeric as a consequence
    )
  ) %>% 
  setNames(files_datetimes)

dates <- map(dates_raw, ~mutate(
  .x,
  time = sprintf("%04d", as.integer(X2)), # otherwise we lose leading zeros
  datetime = ymd_hm(paste0(X1, time)),
  datetime = datetime - hours(2), # Convert from CEST to UTC
  datetime_adj = datetime - minutes(10), # This is necessary to cope with the guidelines about hourly averages mentioned below
  # How often is it actually the case that only a part of the three traps measured pollen at any given time:
  # First we need to make sure that all measurements are exactly registered at 10, 20, 30, 40, 50, 0 minutes after the hour.
  # minute(data_raw %>% bind_rows() %>% pull(datetime)) %>% unique() # It's always plus one to fix it if necessary at all.
  datetime_adj = if_else(minute(datetime_adj) %% 10 != 0, datetime_adj + minutes(1), datetime_adj),
  hour = hour(datetime_adj) + 1,
  hour = if_else(hour == 24, 0, hour),
  date = date(datetime_adj)) %>% 
  select(datetime, date, hour)
  )

# Combining the measurements with the datetimes for the three traps
# It appears that the traps have slightly different observations (e.g. 9 minutes past the full hour instead of 10). This will have no impact, once the concentrations are averaged per hour
data_raw <- map2(data_raw, rep(1:3, each = 4), ~bind_cols(.x, dates[[.y]]) %>% mutate(trap = 2^(.y)))

# Create a new variable to differentiate between counted lines; note that traps were already defined in the previous map-call above
data_raw <- map2(data_raw, rep(c(2, 4, 6, 7), 3), ~mutate(.x, line = .y))

# We are only interested on a subset of the data. The other species either have very low measurements in Switzerland, or they are not relevant from a medical point of view. This selection has been discussed with Regula Gehrig and has been used for a longer time now at MCH.
# The order of variables is different in the PAY2 and PAY6 data, hence we quicly sort them here:
species_selection <- c("Castanea", "Alnus", "Ulmus", "Cupressus", "Fraxinus", "Fagus", "Juglans", "Plantago", "Corylus", 
    "Pinus", "Quercus", "Rumex", "Platanus", "Populus", "Poaceae", "Salix", "Betula", "Carpinus", 
    "Urtica", "Taxus", "Picea")

data_raw <- map(data_raw, ~select(.x, 
    kacastz0, kaalnuz0, kaulmuz0, kacuprz0, kafraxz0, kafaguz0, kajuglz0, khplanz0, kacoryz0, 
    kapinuz0, kaquerz0, khrumez0, kaplatz0, kapopuz0, khpoacz0, kasaliz0, kabetuz0, kacarpz0, 
    khurtiz0, kataxuz0, kapicez0, datetime, date, hour, trap, line) %>% 
  setNames(c(species_selection, "datetime", "date", "hour", "trap", "line")))

```

## Data Bias Correction

The team in Payerne found that the air sucking rates of the Hirst traps are actually higher in reality than reported by the manufacturer. The traps suck in 13.5 l per minute instead of 10 l per minute. Hence our Pollen counts per 10 minutes are too high and should be reduced by a factor of 1.35.

```{r}
data_raw <- map(data_raw, ~mutate_at(.x, vars(all_of(species_selection)), ~./1.35))
```


## Imputation and Timeframe Selection

There are a few missing measurements.
Generally, if one species is NA in one specific trap and line at a certain time, then all species are NA. This usually occurs when the silicone rubber bands were being exchanged.
What is surprising though is the fact that not all lines have the same amount of NAs for the same trap.
This might suggest that some entries were omitted during the manual pollen counting exercise.
If these values are outside of the blooming season then we don't have an issue. This should be further evaluated once a decision was made on which species will be evaluated in the paper.


```{r eval = FALSE}

# # Locate Missing Data (to present to Fiona)
# 
# missing <- data_raw %>% 
#   bind_rows %>% 
#   filter_all(any_vars(is.na(.))) %>% 
#   select(datetime, date, hour, trap, line) 
# 
# missing %>% 
#   write.csv(file = "missing.csv")
# 
# missing %>% 
#   group_by(trap, line, date) %>% 
#   summarise() %>% 
#   ungroup() %>% 
#   mutate(dummy = date) %>% 
#   pivot_wider(values_from = date, names_from = c(trap, line)) %>% 
#   write.csv("overview.csv")

# map2(data, names(data), ~Amelia::missmap(as.data.frame(.x), y.labels = "", y.at = 1, col = swatch()[c(4,2)], main = paste("Missingnes Map for", .y)))
data_raw[[1]] %>% as.data.frame() %>% Amelia::missmap(y.labels = "", y.at = 1, col = swatch()[c(4,2)], main = "Missingness Map for Trap 2 - Line 2")

```

For now we leave these measurements as NA and will not plot them nor impute values for statistical analyses. Only were few values are ipacted after the averages were calculated.

```{r}
# Make sure that all dataframes have the same amount of rows, working with max amount of data available
# It's probably better to work with full days only, so that each measurement independet of the aggregation window has the same meaning
data_raw <- map(data_raw, ~.x %>% filter(datetime > as_datetime("2013-04-02 00:00:00"),
                                         datetime <= as_datetime("2013-06-30 00:00:00"))) 
```

We are mostly interested in observations where Pollen are actually being measured and not the ones where no trap detected anything (which is usually true for many days outside the blooming season). This will lead to a large increase in NA values (that will not be plotted or used for statistical comparison). This is of course a deliberate choice, it was not the goal of the study to see how often all traps measured zero pollen. Everything strongly depends on this decision and the data preparation should be adjusted if other statistical questions are being evaluated.
We might be switching up the data prep after the first meeting to make the data more consistent and the resulting residuals from the model fit easier to work with. So instead of setting value to NA when all traps are zero for any given time, we will set value to zero if at least ONE trap is zero at a given time. This has a potentially large effect on the analysis and should be reevaluated if the code is used for other problems again in the future. Please note that the data with the suffix _raw still contains all the zeros, wheras the corresponding data frames without that suffix contain NAs for all timestemp where no pollen were measured by any trap. As soon as we are calculating common means of the three traps, the problem becomes a bit more complicated: we limit ourselves usually to timesteps where all three traps measured, because otherwise the error function (t-distribution) will be distorted.

```{r}
measurements_to_exclude <- data_raw %>%
  bind_rows %>%
  group_by(trap, datetime) %>%
  replace(is.na(.), 0) %>%
  mutate_at(vars(all_of(species_selection)), .funs = list(exclude = ~mean(.))) %>%
  ungroup() %>%
  group_by(datetime) %>%
  mutate_at(vars(all_of(paste0(species_selection, "_exclude"))), function(x){
      if(all(as.integer(x) == 0))
        return(NA) # This will set all measurements to NA if no trap measured any pollen
      else
        return(0)
  }) %>%
  ungroup()


measurements_after_exclusion <- map2_df(data_raw %>% bind_rows %>% select(all_of(species_selection)), measurements_to_exclude %>% select(contains("exclude")), ~.x + as.integer(.y))

data <- measurements_after_exclusion %>%
  bind_cols(measurements_to_exclude %>%
              select(datetime, date, hour, trap, line)) %>%
  group_split(trap,line)


```

```{r eval = FALSE}
data[[1]] %>%  as.data.frame() %>% Amelia::missmap(y.labels = "", y.at = 1, col = swatch()[c(4,2)], main = "Missingness Map for Trap 2 - Line 2 after data Preparation")

```


## Aggregation

### By Species

In The following five groups were defined based on the Pollen-Grain sizes.

- Group 1: Urtica, Castanea
- Group 2: Alnus, Betula, Corylus, Fraxinus, Platanus, Rumex, Salix
- Group 3: Plantago, Poaceae, Populus, Taxus, Cupressus, Ulmus
- Group 4: Carpinus, Fagus, Juglans, Quercus
- Group 5: Pinus, Picea

```{r}

Group1 <- c("Urtica", "Castanea")
Group2 <- c("Alnus", "Betula", "Corylus", "Fraxinus", "Platanus", "Rumex", "Salix")
Group3 <- c("Plantago", "Poaceae", "Populus", "Taxus", "Cupressus", "Ulmus")
Group4 <- c("Carpinus", "Fagus", "Juglans", "Quercus")
Group5 <- c("Pinus", "Picea")

data_raw <- map(data_raw, ~.x %>% 
  mutate(Group1 = if_else(!is.na(Urtica) | !is.na(Castanea), 
                              rowSums(.[names(.x) %in% Group1], na.rm = TRUE), 
                              NA_real_),
         Group2 = if_else(!is.na(Alnus) | !is.na(Betula) | !is.na(Corylus) | !is.na(Fraxinus) | !is.na(Platanus) | !is.na(Rumex) | !is.na(Salix), 
                              rowSums(.[names(.x) %in% Group2], na.rm = TRUE), 
                              NA_real_),
         Group3 = if_else(!is.na(Plantago) | !is.na(Poaceae) | !is.na(Populus) | !is.na(Taxus) | !is.na(Cupressus) | !is.na(Ulmus), 
                               rowSums(.[names(.x) %in% Group3], na.rm = TRUE), 
                               NA_real_),
         Group4 = if_else(!is.na(Carpinus) | !is.na(Fagus) | !is.na(Juglans) | !is.na(Quercus), 
                                rowSums(.[names(.x) %in% Group4], na.rm = TRUE), 
                                NA_real_),
         Group5 = if_else(!is.na(Pinus) | !is.na(Picea),
                                rowSums(.[names(.x) %in% Group5], na.rm = TRUE), 
                                NA_real_),
         Total = if_else(!is.na(Group1) | !is.na(Group2) | !is.na(Group3) | !is.na(Group4) | !is.na(Group5), 
                         rowSums(.[names(.x) %in% species_selection], na.rm = TRUE), 
                         NA_real_)))
```


Five concentration classes could also be defined for the analysis: 0-9 pollen grains/m3, 10-19 pollen grains/m3, 20-49 pollen grains/m3, 50-99 pollen grains/m3, 100-299 grains/m3, and >=300 pollen grains/m3, with the aim of better understanding whether different pollen concentrations have an impact on sampling and measurement error. This will be done on a ad-hoc basis when needed further below. The classes will depend on the average window.

### Temporal

One of the questions is, how long the observation period should be for the Pollen traps to produce robust results on any specific day.
The original time span is 10 minutes, we want to look at various average concentration (e.g. hourly, 3 hours, 6 hours, daily).

There are some general guidelines from Regula Gehrig on how to average concentration that should be followed here.

Die Stundenwerte werden wie folgt aus den 10-Minutenwerten berechnet:
z.B. Wert für 6:00 Uhr UTC wird berechnet aus 05:10 – 6:00 UTC

Die 3-h-Werte, die du auch im DWH findest, werden so berechnet:
z.B. Wert um 09:00 UTC: 06:10-09:00

Die Tageswerte werden bei den Pollen glaub ich aus den Stundenwerten berechnet aus:
01:00 – 24:00 (d.h. 00:00 – im DWH gibt es keinen Wert 24:00) UTC 

We should apply the same setting to NA as above for all averages. But AFTER the averaging was carried out, in order to include zero measurements in the mean, otherwise we overestimate the means by removing NAs.

```{r}

# Hour 1 represents the duration 00:10 - 01:00, and same for all hours up to 24
data_hourly_raw <- map(data_raw, ~.x %>% 
  mutate(date = if_else(hour == 0, date + days(1), date)) %>%  # This way they will be allocated to the next day)
  group_by(date, hour, trap, line) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~mean(., na.rm = TRUE)) %>% 
  ungroup())
data_hourly <- set_na(data_hourly_raw)

data_hours2_raw <- map(data_raw, ~.x %>% 
  mutate(hms = format(datetime, "%H:%M:%S"),
    hour = case_when(
      hms > "00:00:00" & hms <= "02:00:00" ~ 2,
      hms > "02:00:00" & hms <= "04:00:00" ~ 4,
      hms > "04:00:00" & hms <= "06:00:00" ~ 6,
      hms > "06:00:00" & hms <= "08:00:00" ~ 8,
      hms > "08:00:00" & hms <= "10:00:00" ~ 10,
      hms > "10:00:00" & hms <= "12:00:00" ~ 12,
      hms > "12:00:00" & hms <= "14:00:00" ~ 14,
      hms > "14:00:00" & hms <= "16:00:00" ~ 16,
      hms > "16:00:00" & hms <= "18:00:00" ~ 18,
      hms > "18:00:00" & hms <= "20:00:00" ~ 20,
      hms > "20:00:00" & hms <= "22:00:00" ~ 22,
      hms > "22:00:00" | hms == "00:00:00" ~ 0 # For the next day already
      ),
    date = if_else(hour == 0, date + days(1), date) # This way they will be allocated to the next day
  ) %>% 
  group_by(date, hour, trap, line) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~mean(., na.rm = TRUE)) %>% 
  ungroup())

data_hours2 <- set_na(data_hours2_raw)


data_hours3_raw <- map(data_raw, ~.x %>% 
  mutate(hms = format(datetime, "%H:%M:%S"),
    hour = case_when(
      hms > "00:00:00" & hms <= "03:00:00" ~ 3,
      hms > "03:00:00" & hms <= "06:00:00" ~ 6,
      hms > "06:00:00" & hms <= "09:00:00" ~ 9,
      hms > "09:00:00" & hms <= "12:00:00" ~ 12,
      hms > "12:00:00" & hms <= "15:00:00" ~ 15,
      hms > "15:00:00" & hms <= "18:00:00" ~ 18,
      hms > "18:00:00" & hms <= "21:00:00" ~ 21,
      hms > "21:00:00" | hms == "00:00:00" ~ 0 # For the next day already
      ),
    date = if_else(hour == 0, date + days(1), date) # This way they will be allocated to the next day
  ) %>% 
  group_by(date, hour, trap, line) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~mean(., na.rm = TRUE)) %>% 
  ungroup())

data_hours3 <- set_na(data_hours3_raw)

data_hours6_raw <- map(data_raw, ~.x %>% 
  mutate(hms = format(datetime, "%H:%M:%S"),
    hour = case_when(
      hms > "00:00:00" & hms <= "06:00:00" ~ 6,
      hms > "06:00:00" & hms <= "12:00:00" ~ 12,
      hms > "12:00:00" & hms <= "18:00:00" ~ 18,
      hms > "18:00:00" | hms == "00:00:00" ~ 0
    ),
    date = if_else(hour == 0, date + days(1), date)
  ) %>% 
  group_by(date, hour, trap, line) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~mean(., na.rm = TRUE)) %>% 
  ungroup())

data_hours6 <- set_na(data_hours6_raw)

data_hours12_raw <- map(data_raw, ~.x %>% 
  mutate(hms = format(datetime, "%H:%M:%S"),
    hour = case_when(
      hms > "00:00:00" & hms <= "12:00:00" ~ 12,
      hms > "12:00:00" | hms == "00:00:00" ~ 0
    ),
    date = if_else(hour == 0, date + days(1), date)
  )  %>% 
  group_by(date, hour, trap, line) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~mean(., na.rm = TRUE)) %>% 
  ungroup())

data_hours12 <- set_na(data_hours12_raw)

data_daily_raw <- map(data_hourly_raw, ~.x %>% 
                    mutate(date = if_else(hour == 0, date - days(1), date)) %>% 
                    group_by(date, trap, line) %>% 
                    summarise_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~mean(., na.rm = TRUE)) %>% 
                    mutate(hour = 23) %>% # Random hour here 
                    ungroup())

data_daily <- set_na(data_daily_raw)

data_daily_raw <- map(data_daily_raw, ~.x %>% 
  mutate(hour = 23,
         timestamp = ymd_hm(paste0(as.character(date), paste0(sprintf("%02d", hour), ":59")))))

```

### Robustness of Traps and Missing Values

We want to assess the similarity between traps for different temporal averages. It is known / expected that Hirst traps are not reliable for short time-windows. The statistics calculated below, are affected by too many measurements that are zero for any given time. But at the same time, we will also compare those Hirst traps to newer automatic traps with a high temporal resolution in a second vignette. It is therefore, crucial to understand how robust shorter observation-windows are for the Hirst traps. We basically want to find the shortest time interval that is still robust. 

Note: It does not matter whether we use the prepared or raw data for this, the outcome will be the same (because we include all measurements where at least one trap measured, so only zeros were removed where none measured. Those "none measured" are anyways defined by the complete obersvation period from April to July).


```{r eval = TRUE}
data_aggs <- list(data_hours2, data_hours3, data_hours6, data_hours12, data_daily)
number_of_measurements <- map(data_aggs, ~.x %>% 
  bind_rows %>%   
  group_by(trap, date, hour) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~mean(., na.rm = TRUE)) %>% 
  replace(is.na(.), 0) %>% 
  # mutate_at(vars(all_of(species_selection)), ~if_else((. >= 10), ., 0)) %>% 
  mutate_at(vars(all_of(species_selection), Total), ~(as.integer(as.integer(.) != 0))) %>% 
  ungroup %>% 
  group_by(date, hour) %>%
  summarise_at(vars(all_of(species_selection), Total), ~sum(., na.rm = TRUE)) %>% 
  ungroup)

timesteps <- map(data_aggs, ~ .x[[1]] %>% nrow())

number_traps <- map2(number_of_measurements, timesteps, ~ bind_rows(.x %>% 
    summarise_at(vars(all_of(species_selection), Total), ~sum(. == 3)),  
  .x %>% 
    summarise_at(vars(all_of(species_selection), Total), ~sum(. == 2)),
  .x %>% 
    summarise_at(vars(all_of(species_selection), Total), ~sum(. == 1)),
  .x %>% 
  summarise_at(vars(all_of(species_selection), Total), ~sum(. == 0))
  ) %>% 
  mutate(Comment = c("All traps measured", "Two traps measured", "One trap measured", "None measured")) %>%
  mutate_at(vars(all_of(species_selection), Total), ~ scales::percent(round(. / !!.y, 2))) %>% 
  select(Comment, Fraxinus, Poaceae, Betula, Quercus, Pinus, Total))

number_traps %>% 
  bind_rows() %>% 
  kable() %>% 
  kable_styling("striped") %>% 
  pack_rows("2-Hour Averages", 1, 4) %>% 
  pack_rows("3-Hour Averages", 5, 8) %>% 
  pack_rows("6-Hour Averages", 9, 12) %>% 
  pack_rows("12-Hour Averages", 13, 16) %>% 
  pack_rows("Daily Averages", 17, 20) %>% 
  column_spec(c(1, 6), border_right = TRUE)

# For rendering docx documents
# NOT ALLOWED AT MCH TO EXECUTE .exe file
# save_kable(table_traps, file = "table_traps.png")
# webshot::install_phantomjs()

 

```

```{r eval = TRUE}
number_traps <- map(number_of_measurements, ~ bind_rows(.x %>% 
    summarise_at(vars(all_of(species_selection), Total), ~sum(. == 3)),  
  .x %>% 
    summarise_at(vars(all_of(species_selection), Total), ~sum(. == 2)),
  .x %>% 
    summarise_at(vars(all_of(species_selection), Total), ~sum(. == 1))) %>% 
  select(Fraxinus, Poaceae, Betula, Quercus, Pinus, Total))

number_of_timesteps <- map(number_traps, ~colSums(.x %>% select(Fraxinus:Total)))
timesteps_tb <- map(number_of_timesteps, ~bind_rows(.x, .x, .x))

number_traps_perc <- map2(number_traps, timesteps_tb, ~ .x / .y) %>% 
  map(~.x %>% 
        mutate_all( ~ scales::percent(round(., 2))) %>%
        mutate(Comment = c("All traps measured", "Two traps measured", "One trap measured")) %>%
        select(Comment, Fraxinus, Poaceae, Betula, Quercus, Pinus, Total))

kable_number_traps <- number_traps_perc %>% 
  bind_rows() %>% 
  kable() %>% 
  kable_styling("striped") %>% 
  pack_rows("2-Hour Averages", 1, 3) %>% 
  pack_rows("3-Hour Averages", 4, 6) %>% 
  pack_rows("6-Hour Averages", 7, 9) %>% 
  pack_rows("12-Hour Averages", 10, 12) %>% 
  pack_rows("Daily Averages", 13, 15) %>% 
  column_spec(c(1, 6), border_right = TRUE)

kable_number_traps
  

```

```{r}
min_measurement <- map(c(list(data_raw, data_hourly), data_aggs), ~.x %>% 
  bind_rows %>% 
  select(Total) %>%  
  filter(Total > 0) %>% 
  min())

min_measurement_tb <- tibble(Min_Measurement = min_measurement %>% unlist() %>% round(2),
                             Resolution = c("10-Minutes", "Hourly", "2-Hour", "3-Hour", "6-Hour", "12-Hour", "Daily"))
min_measurement_tb %>% 
  kable %>% 
  kable_styling("striped", full_width = FALSE)

```



So it seems that it actually happens often that only a fraction of the traps is measuring pollen. It can be expected that the discrepancies mostly occur during times with low pollen concentrations. For that reason, we can filter everything below 10 pollen /m^3 and see how the percentages change. But the results don't change dramatically after doing so. Further evaluations concerning this minimal threshold are carried out below.


```{r}
# How many two hour average concentrations are NA for bi-hourly values?

# map(data_hours2_raw, ~.x %>% filter(is.na(Total)))

# There are only three missing concentrations for 2-hour averages and only in one line per trap. Hence we have no missing measurements for the highest temporal resolution.
```


# Tabular Comparison of Various Metrics

In the following when we look at measurements from traps, we simply average between the four counted lines.

Several metrics were calculated: the frequency of occurrence for each pollen was obtained by counting the number of days that each particular pollen was detected; the seasonal mean was calculated by averaging values for each pollen for all the days that it occurred; and the Seasonal Pollen Integral (SPI) was calculated by integrating the concentrations of a particular pollen over the entire season (Mandrioli et al., 1998).

These metrics are well-known and usually part of a Pollen Measurement study. It is common to calculate the means before setting values outside the blooming season to NA. Hence I created a data_raw list of tibbles above just for that purpose.

```{r }

occurence_raw <- data_daily_raw %>% 
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~sum(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~sum(. !=0, na.rm = TRUE)) %>% 
  ungroup() 

names <- names(occurence_raw)

occurence_raw <- occurence_raw %>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  select(name, trap2, trap4, trap8) %>% 
  filter(name != "trap")

maximum_raw <- data_daily_raw %>% 
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~max(., na.rm = TRUE)) %>% 
  ungroup()%>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  filter(name != "trap") %>% 
  mutate_at(vars(starts_with("trap")), round)

average_raw <- data_daily_raw %>% 
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~mean(., na.rm = TRUE)) %>% 
  ungroup()%>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  filter(name != "trap") %>% 
  mutate_at(vars(starts_with("trap")), round)

spi_raw <- data_daily_raw %>% # Seasonal Pollen Integral
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~sum(., na.rm = TRUE)) %>% 
  ungroup()%>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  filter(name != "trap") %>% 
  mutate_at(vars(starts_with("trap")), round)

kable_metrics <- occurence_raw %>% 
  inner_join(maximum_raw, by = "name") %>% 
  inner_join(average_raw, by = "name") %>% 
  inner_join(spi_raw, by = "name") %>% 
  filter(name %in% c(species_selection)) %>% 
  arrange(desc(trap8.y.y)) %>% 
  setNames(c("Name", rep(paste0("trap", c(2, 4, 8)), times = 4))) %>% 
  kable(escape = FALSE) %>%
  kable_styling(c("striped", "condensed"), full_width = FALSE, font_size = 20) %>% 
  add_header_above(c(" " = 1, "Frequency of Occurence [#Days]" = 3, "Maximum Daily Concentration [Pollen/m³]" = 3, "Average Daily Concentration [Pollen/m³]" = 3, "Seasonal Pollen Integral [Pollen/m³]" = 3)) %>% 
  column_spec(1, italic = TRUE) %>% 
  column_spec(2:13, width = "2.5cm") %>% 
  column_spec(seq(1, 13, 3), border_right = TRUE)

kable_metrics

```

## Variability of the Means

This is a simple analysis of the metrics above, showing how much the means differ between the traps.

```{r}

maximum_raw_hours2 <- data_hours2_raw %>% 
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~max(., na.rm = TRUE)) %>% 
  ungroup()%>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  filter(name != "trap") %>% 
  mutate_at(vars(starts_with("trap")), round)

average_raw_hours2<- data_hours2_raw %>% 
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~mean(., na.rm = TRUE)) %>% 
  ungroup()%>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  filter(name != "trap") %>% 
  mutate_at(vars(starts_with("trap")), round)

sd_list_daily <- list(maximum_raw, average_raw)
sd_list_hours2 <- list(maximum_raw_hours2, average_raw_hours2)

sd_tb_daily <- map(sd_list_daily, ~.x %>% 
  pivot_longer(cols = trap2:trap8, names_to = "trap", values_to = "value") %>% 
  group_by(name) %>% 
  mutate(sd = sd(value),
         se = sd / 3,
         mean = mean(value),
         cv = sd/mean) %>% 
  filter(trap == "trap2",
         name %in% c("Total", "Fraxinus", "Poaceae", "Betula", "Quercus", "Pinus")) %>% 
  select(name, sd, mean, se, cv, mean)) %>% 
  bind_rows %>% 
  ungroup
  
sd_tb_hours2 <- map(sd_list_hours2, ~.x %>% 
  pivot_longer(cols = trap2:trap8, names_to = "trap", values_to = "value") %>% 
  group_by(name) %>% 
  mutate(sd = sd(value),
         se = sd / 3,
         mean = mean(value),
         cv = sd/mean) %>% 
  filter(trap == "trap2",
         name %in% c("Total", "Fraxinus", "Poaceae", "Betula", "Quercus", "Pinus")) %>% 
  select(name, sd, mean, se, cv, mean)) %>% 
  bind_rows %>% 
  ungroup

sd_tb_hours2 %>% 
  bind_cols(sd_tb_daily) %>% 
  mutate_if(is.numeric, ~round(., 2)) %>% 
  select(-name...6) %>% 
  setNames(c(names(sd_tb_hours2), names(sd_tb_hours2 %>% select(-name)))) %>% 
  kable() %>% 
  kable_styling("striped", full_width = FALSE, font_size = 16) %>% 
  add_header_above(c(" " = 1, "Two-Hour Averages" = 4, "Daily Averages" = 4)) %>% 
  column_spec(1, border_right = TRUE) %>% 
  column_spec(5, border_right = TRUE) %>% 
  pack_rows(start_row = 1, end_row = 6, group_label = "Max. Value") %>% 
  pack_rows(start_row = 7, end_row = 12, group_label = "Avg. Value") 



```

## Variability of the Individual Measurements

These tables nicely show how large the spread becomes in the data looking at the coefficient of variance (sd / mean) for two-hourly and daily values. The selection of two-hourly and daily values was carried out after discussing the paper draft. It seems quite common in the literature to use 2hour averages as the shortest interval and daily as the longest. Hence we followed this approach to show the two extremes. One can see how the 2-hour values have a larger cv.

### Top Species

```{r}

sd_2hour_ind <- data_hours2_raw %>% 
  bind_rows %>% 
  group_by(date, hour, trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  pivot_longer(Castanea:Total, names_to = "name", values_to = "value") %>% 
  group_by(name) %>% 
  mutate(sd_ind = sd(value),
         mean_ind = mean(value),
         se_ind = sd_ind / n(),
         cv_ind = sd_ind/mean_ind) %>% 
    filter(name %in% c("Total", "Fraxinus", "Poaceae", "Betula", "Quercus", "Pinus"),
         trap == 2,
         date == as.Date("2013-04-02"),
         hour == 2) %>% 
  select(name, sd_ind:cv_ind) %>% 
  bind_rows %>% 
  ungroup

sd_daily_ind <- data_daily_raw %>% 
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  pivot_longer(Castanea:Total, names_to = "name", values_to = "value") %>% 
  group_by(name) %>% 
  mutate(sd_ind = sd(value),
         mean_ind = mean(value),
         se_ind = sd_ind / n(),
         cv_ind = sd_ind/mean_ind) %>% 
    filter(name %in% c("Total", "Fraxinus", "Poaceae", "Betula", "Quercus", "Pinus"),
         trap == 2,
         date == as.Date("2013-04-02")) %>% 
  select(name, sd_ind:cv_ind) %>% 
  bind_rows %>% 
  ungroup

kable_cv_top <- sd_2hour_ind %>% 
  bind_cols(sd_daily_ind) %>% 
  mutate_if(is.numeric, ~round(., 2)) %>% 
  select(-name...6) %>% 
  setNames(c(names(sd_tb_hours2), names(sd_tb_hours2 %>% select(-name)))) %>% 
  kable() %>% 
  kable_styling("striped", full_width = FALSE, font_size = 16) %>% 
  add_header_above(c(" " = 1, "Two-Hour Averages" = 4, "Daily Averages" = 4)) %>% 
  column_spec(1, border_right = TRUE) %>% 
  column_spec(5, border_right = TRUE)

kable_cv_top

```

### Pollen Size Groups

```{r}

groups <- c("Group1", "Group2", "Group3", "Group4", "Group5", "Total")

sd_2hour_ind <- data_hours2_raw %>% 
  bind_rows %>% 
  group_by(date, hour, trap) %>% 
  summarise_at(vars(all_of(species_selection), all_of(groups)), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  pivot_longer(Castanea:Group5, names_to = "name", values_to = "value") %>% 
  group_by(name) %>% 
  mutate(sd_ind = sd(value),
         mean_ind = mean(value),
         se_ind = sd_ind / n(),
         cv_ind = sd_ind/mean_ind) %>% 
    filter(name %in% groups,
         trap == 2,
         date == as.Date("2013-04-02"),
         hour == 2) %>% 
  select(name, sd_ind:cv_ind) %>% 
  bind_rows %>% 
  ungroup

sd_daily_ind <- data_daily_raw %>% 
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection), all_of(groups)), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  pivot_longer(Castanea:Group5, names_to = "name", values_to = "value") %>% 
  group_by(name) %>% 
  mutate(sd_ind = sd(value),
         mean_ind = mean(value),
         se_ind = sd_ind / n(),
         cv_ind = sd_ind/mean_ind) %>% 
    filter(name %in% groups,
         trap == 2,
         date == as.Date("2013-04-02")) %>% 
  select(name, sd_ind:cv_ind) %>% 
  bind_rows %>% 
  ungroup

kable_cv_size <- sd_2hour_ind %>% 
  bind_cols(sd_daily_ind) %>% 
  mutate_if(is.numeric, ~round(., 2)) %>% 
  select(-name...6) %>% 
  setNames(c(names(sd_tb_hours2), names(sd_tb_hours2 %>% select(-name)))) %>% 
  kable() %>% 
  kable_styling("striped", full_width = FALSE, font_size = 16) %>% 
  add_header_above(c(" " = 1, "Two-Hour Averages" = 4, "Daily Averages" = 4)) %>% 
  column_spec(1, border_right = TRUE) %>% 
  column_spec(5, border_right = TRUE)



```

### Concentration Groups

```{r}

concs <- c("Group00_10", "Group10_20", "Group20_50", "Group50_100", "Group100_300", "Group300")

data_hour2_conc <- data_hours2_raw %>% 
  bind_rows %>% 
  group_by(date, hour, trap) %>% 
  summarise_at(vars(all_of(species_selection), all_of(groups)), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  pivot_longer(Castanea:Picea, names_to = "type", values_to = "value") %>% 
  select(trap, date, hour, type, value) %>% 
  pivot_wider(names_from = trap) %>% 
  setNames(c("date", "hour", "type", "trap2", "trap4", "trap8")) %>% 
  mutate(measuring_traps = as.integer(!is.na(trap2)) + as.integer(!is.na(trap4)) + as.integer(!is.na(trap8)),
         mean = if_else(!is.na(trap2) | !is.na(trap4) | !is.na(trap8), # For the density plots we want to see all observations, even if the mean was calculated based on only one trap.
                          rowSums(.[4:6], na.rm = TRUE) / measuring_traps, 
                          NA_real_)) %>% 
  pivot_longer(trap2:trap8, names_to = "trap") %>% 
  mutate(conc = case_when(
    mean >= 0 & mean < 10 ~ "Group00_10",
    mean >= 10 & mean < 20 ~ "Group10_20",
    mean >= 20 & mean < 50 ~ "Group20_50",
    mean >= 50 & mean < 100 ~ "Group50_100",
    mean >= 100 & mean < 300 ~ "Group100_300",
    mean >= 300 ~ "Group300"
  )) %>% 
  select(date, hour, trap, value, conc, type) %>% 
  pivot_wider(names_from = conc, values_from = value)

data_daily_conc <- data_daily_raw %>% 
  bind_rows %>% 
  group_by(date, hour, trap) %>% 
  summarise_at(vars(all_of(species_selection), all_of(groups)), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  pivot_longer(Castanea:Picea, names_to = "type", values_to = "value") %>% 
  select(trap, date, hour, type, value) %>% 
  pivot_wider(names_from = trap) %>% 
  setNames(c("date", "hour", "type", "trap2", "trap4", "trap8")) %>% 
  mutate(measuring_traps = as.integer(!is.na(trap2)) + as.integer(!is.na(trap4)) + as.integer(!is.na(trap8)),
         mean = if_else(!is.na(trap2) | !is.na(trap4) | !is.na(trap8), # For the density plots we want to see all observations, even if the mean was calculated based on only one trap.
                          rowSums(.[4:6], na.rm = TRUE) / measuring_traps, 
                          NA_real_)) %>% 
  pivot_longer(trap2:trap8, names_to = "trap") %>% 
  mutate(conc = case_when(
    mean >= 0 & mean < 10 ~ "Group00_10",
    mean >= 10 & mean < 20 ~ "Group10_20",
    mean >= 20 & mean < 50 ~ "Group20_50",
    mean >= 50 & mean < 100 ~ "Group50_100",
    mean >= 100 & mean < 300 ~ "Group100_300",
    mean >= 300 ~ "Group300"
  )) %>% 
  select(date, hour, trap, value, conc, type) %>% 
  pivot_wider(names_from = conc, values_from = value)

sd_2hour_ind <- data_hour2_conc %>% 
  bind_rows %>% 
  group_by(date, hour, trap) %>% 
  summarise_at(all_of(concs), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  pivot_longer(Group00_10:Group300, names_to = "name", values_to = "value") %>% 
  group_by(name) %>% 
  mutate(sd_ind = sd(value, na.rm = TRUE),
         mean_ind = mean(value, na.rm = TRUE),
         se_ind = sd_ind / n(),
         cv_ind = sd_ind/mean_ind) %>% 
    filter(name %in% concs,
         trap == "trap2",
         date == as.Date("2013-04-02"),
         hour == 2) %>% 
  select(name, sd_ind:cv_ind) %>% 
  bind_rows %>% 
  ungroup

sd_daily_ind <- data_daily_conc %>% 
  bind_rows %>% 
  group_by(date, hour, trap) %>% 
  summarise_at(all_of(concs), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  pivot_longer(Group00_10:Group300, names_to = "name", values_to = "value") %>% 
  group_by(name) %>% 
  mutate(sd_ind = sd(value, na.rm = TRUE),
         mean_ind = mean(value, na.rm = TRUE),
         se_ind = sd_ind / n(),
         cv_ind = sd_ind/mean_ind) %>% 
    filter(name %in% concs,
         trap == "trap2",
         date == as.Date("2013-04-02"),
         hour == 23) %>% 
  select(name, sd_ind:cv_ind) %>% 
  bind_rows %>% 
  ungroup

kable_cv_conc <- sd_2hour_ind %>% 
  bind_cols(sd_daily_ind) %>% 
  mutate_if(is.numeric, ~round(., 2)) %>% 
  select(-name...6) %>% 
  setNames(c(names(sd_tb_hours2), names(sd_tb_hours2 %>% select(-name)))) %>% 
  kable() %>% 
  kable_styling("striped", full_width = FALSE, font_size = 16) %>% 
  add_header_above(c(" " = 1, "Two-Hour Averages" = 4, "Daily Averages" = 4)) %>% 
  column_spec(1, border_right = TRUE) %>% 
  column_spec(5, border_right = TRUE)

kable_cv_conc

```

```{r}

maximum_raw_hours2 <- data_hours2%>% 
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~max(., na.rm = TRUE)) %>% 
  ungroup()%>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  filter(name != "trap") %>% 
  mutate_at(vars(starts_with("trap")), round)

average_raw_hours2 <- data_hours2 %>% 
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~mean(., na.rm = TRUE)) %>% 
  ungroup()%>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  filter(name != "trap") %>% 
  mutate_at(vars(starts_with("trap")), round)

maximum_raw_daily <- data_daily %>% 
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~max(., na.rm = TRUE)) %>% 
  ungroup()%>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  filter(name != "trap") %>% 
  mutate_at(vars(starts_with("trap")), round)

average_raw_daily <- data_daily %>% 
  bind_rows %>% 
  group_by(date, trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~mean(., na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(trap) %>% 
  summarise_at(vars(all_of(species_selection), Total), ~mean(., na.rm = TRUE)) %>% 
  ungroup()%>% 
  data.table::transpose() %>% 
  setNames(paste0("trap", c(2, 4, 8))) %>% 
  mutate(name = names) %>% 
  filter(name != "trap") %>% 
  mutate_at(vars(starts_with("trap")), round)

sd_list_daily <- list(maximum_raw_daily, average_raw_daily)
sd_list_hours2 <- list(maximum_raw_hours2, average_raw_hours2)

sd_tb_daily <- map(sd_list_daily, ~.x %>% 
  pivot_longer(cols = trap2:trap8, names_to = "trap", values_to = "value") %>% 
  group_by(name) %>% 
  mutate(sd = sd(value),
         se = sd / 3,
         mean = mean(value),
         cv = sd/mean) %>% 
  filter(trap == "trap2",
         name %in% c("Total", "Fraxinus", "Poaceae", "Betula", "Quercus", "Pinus")) %>% 
  select(name, sd, mean, se, cv, mean)) %>% 
  bind_rows %>% 
  ungroup
  
sd_tb_hours2 <- map(sd_list_hours2, ~.x %>% 
  pivot_longer(cols = trap2:trap8, names_to = "trap", values_to = "value") %>% 
  group_by(name) %>% 
  mutate(sd = sd(value),
         se = sd / 3,
         mean = mean(value),
         cv = sd/mean) %>% 
  filter(trap == "trap2",
         name %in% c("Total", "Fraxinus", "Poaceae", "Betula", "Quercus", "Pinus")) %>% 
  select(name, sd, mean, se, cv, mean)) %>% 
  bind_rows %>% 
  ungroup

sd_tb_hours2 %>% 
  bind_cols(sd_tb_daily) %>% 
  mutate_if(is.numeric, ~round(., 2)) %>% 
  select(-name...6) %>% 
  setNames(c(names(sd_tb_hours2), names(sd_tb_hours2 %>% select(-name)))) %>% 
  kable() %>% 
  kable_styling("striped", full_width = FALSE, font_size = 16) %>% 
  add_header_above(c(" " = 1, "Two-Hour Averages" = 4, "Daily Averages" = 4)) %>% 
  column_spec(1, border_right = TRUE) %>% 
  column_spec(5, border_right = TRUE) %>% 
  pack_rows(start_row = 1, end_row = 5, group_label = "Max. Value") %>% 
  pack_rows(start_row = 6, end_row = 10, group_label = "Avg. Value") 



```


```{r}


data_aggs <- list(data_hours2, data_hours3, data_hours6, data_hours12, data_daily)
names(data_aggs) <- c("data_hours2", "data_hours3", "data_hours6", "data_hours12", "data_daily")

data_anova_contr <- map(data_aggs, ~map(.x, ~.x %>%
  mutate(timestamp = ymd_hm(paste0(date, hour)))))

data_contr <- map(data_anova_contr, ~.x %>% 
  bind_rows() %>% 
  mutate(trap = as.factor(trap)) %>% 
  group_by(trap, timestamp) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~mean(., na.rm = TRUE)) %>% 
  ungroup)

data_contr_tf <- map(data_contr, ~.x %>% 
  mutate_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~log(. + 1)))

sd_temp <- map(data_contr, ~ .x %>% 
  summarise_at(vars(c("Total", "Fraxinus", "Poaceae", "Betula", "Quercus", "Pinus")), sd, na.rm = TRUE)) %>% 
  bind_rows %>% 
  mutate(Average = c("2-Hour", "3-Hour", "6-Hour", "12-Hour", "Daily")) %>% 
  select(Average, everything())

mean_temp <- map(data_contr, ~ .x %>% 
  summarise_at(vars(c("Total", "Fraxinus", "Poaceae", "Betula", "Quercus", "Pinus")), mean, na.rm = TRUE)) %>% 
  bind_rows %>% 
  mutate(Average = c("2-Hour", "3-Hour", "6-Hour", "12-Hour", "Daily")) %>% 
  select(Average, everything())

sd_mean_temp <- sd_temp %>% 
  bind_cols(mean_temp) %>% 
  mutate(Average = Average...1,
         Total = Total...2 / Total...9,
         Fraxinus = Fraxinus...3 / Fraxinus...10,
         Poaceae = Poaceae...4 / Poaceae...11,
         Betula = Betula...5 / Betula...12,
         Quercus = Quercus...6 / Quercus...13,
         Pinus = Pinus...7 / Pinus...14
         ) %>% 
  select(Average:Pinus)

sd_temp %>% 
  bind_rows(mean_temp) %>% 
  bind_rows(sd_mean_temp) %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  kable(escape = TRUE) %>% 
  kable_styling(full_width = FALSE, font_size = 12) %>% 
  pack_rows(start_row = 1, end_row = 5, group_label = "SD") %>% 
  pack_rows(start_row = 6, end_row = 10, group_label = "MEAN") %>% 
  pack_rows(start_row = 11, end_row = 15, group_label = "SD/MEAN")

```


As described above we averaged the Pollen Counts from the 4 lines per trap and can now compare the traps based on Various Metrics.
Typically we see that trap 8 shows higher measurements for a variety of species. Once the species and temporal-average are fixed, this can be discussed in more detail.

# Visualization

Now let's plot some time series to get a better overview of the data. In the plot below the user can select any species and temporal resolution to compare the measurements from different lines and traps. After that we always average between the lines for each trap, to obtain a representative sample (~10% of pollen-plate evaluated under microscope). As a very general statement, we see again that trap number 8 has higher concentration-measurements than the other two. The shorter the temporal averaging window the large those differences can become. Generally, the analysis tends to be more stable when looking at larger time-windows and Total-Pollen counts. This is not surprising, as these setting allow us to benefit from all the data we have and missing measurements will be excluded during the averaging of concentrations in a time-window. The standard deviation is also largest for trap number 8 for many species.

```{r }
ggthemr("fresh")

gg_comparison <- plot_hirst(species = species, resolution = resolution, group = "trap", traps = c(2, 4, 8), rm_zeros = TRUE, combined = TRUE)

gg_timeseries <- plot_hirst(species = species, resolution = resolution, group = "trap", traps = c(2, 4, 8), rm_zeros = TRUE, combined = FALSE)[[1]] +
  theme(legend.position = "right")

gg_comparison


```





# Statistical Comparison

The analysis should also serve as a base for a scientific publication. We want to assess the robustness and similarity of the Hirst traps. For that purpose we will compare the measured Pollen concentrations for the three different traps. We will assess the similarity between the three traps and test their robustness for different averaging windows. This task of comparing measures has been discussed in scientific literature for many decades and it is not an easy task. I will apply many different approaches to visually and mathematically check the similarity between the three measurements and will add some remarks about their pros, cons and underlying assumptions.. It is in the nature of the problem that no conclusion will be made at the end that holds with a 100% certainty. 

Another goal of this analysis is to set up a pipeline (i.e. code construct) that can be sued to compare newer automatic pollen traps to the traditional Hirst traps. For that reason I try to keep the code as flexible and well structured as possible.

These settings are crucial and should always be remembered when running the chunks below:

- The temporal resolution for the calculation of the mean concentrations
- The species or group of Pollen concentrations, to distinguish between high and low blooming season
- The threshold below which Pollen measurements are set to NA (because they become unreliable) is set to 0 currently

We will first visually evaluate the similarity using Altman-Bland and Correlation Plots and then carry out some statistical tests.
There are two pathways to evaluate the similarity of the measurements between the traps (measures). One can either transform the measurements (log), so that they are normally distributed and then work with ANOVA, F-Test and Tukey Honestly Significance Difference (HSD); or one can take the measurements as is (counts per m³) and work with the non-parametric Kruskal-Wallis Test that only requires rank-symmetry but doesn't assume any underlying distribution of the data. For the non-parametric contrasts multiple options present themselves that we will investigate below.

```{r}

if (resolution == "daily"){
  data_anova <- data_daily
} else if (resolution == "12hour"){
  data_anova <- data_hours12
} else if (resolution == "6hour"){
  data_anova <- data_hours6
} else if (resolution == "3hour"){
  data_anova <- data_hours3
} else if (resolution == "2hour"){
  data_anova <- data_hours2
} else if (resolution == "hourly"){
  data_anova <-data_hourly
}

data_anova <- map(data_anova, ~.x %>%
  mutate(timestamp = ymd_hm(paste0(date, hour))))

data_anova <- data_anova %>% 
  bind_rows() %>% 
  mutate(trap = as.factor(trap)) %>% 
  group_by(trap, timestamp) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~mean(., na.rm = TRUE)) %>% 
  ungroup 

data_transformed <- data_anova %>% 
  mutate_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~log(. + 1))
  
# There are various methods to deal with zeros during log transformation
# 
#     Add a constant value © to each value of variable then take a log transformation - i will add x + 1
#     Impute zero value with mean. - not evaluated
#     Take square root instead of log for transformation - does not really help much

```


## Correlation

The correlation between the traps can be calculated easily and then the CI and p-values must be adjusted for multiple comparison.
The corr-test function from the psych handily offers this functionality.

Careful the correlation coefficients method have some serious shortcomings:

The correlation coefficient measures linear agreement--whether the measurements go up-and-down together. Certainly, we want the measures to go up-and-down together, but the correlation coefficient itself is deficient in at least three ways as a measure of agreement. (http://www.jerrydallal.com/LHSP/compare.htm)

- The correlation coefficient can be close to 1 (or equal to 1!) even when there is considerable bias between the two methods. For example, if one method gives measurements that are always 10 units higher than the other method, the correlation will be 1 exactly, but the measurements will always be 10 units apart.
- The magnitude of the correlation coefficient is affected by the range of subjects/units studied. The correlation coefficient can be made smaller by measuring samples that are similar to each other and larger by measuring samples that are very different from each other. The magnitude of the correlation says nothing about the magnitude of the differences between the paired measurements which, when you get right down to it, is all that really matters.
- The usual significance test involving a correlation coefficient-- whether the population value is 0--is irrelevant to the comparability problem. What is important is not merely that the correlation coefficient be different from 0. Rather, it should be close to (ideally, equal to) 1! 

### Pearson, Spearman and Kendall Correlation

A good summary of the methods and their shortcomings can be found here: https://www.statisticssolutions.com/correlation-Pearson-Kendall-spear man/

Generally the traps show a high level of correlation / association (well above 0.5). It looks like small concentrations lead to the largest discrepancies between the traps.

```{r}

methods <- c("pearson", "spearman", "kendall")

data_corr <- data_transformed %>% # For the robust methods (spearman, kendall) it doesn't matter whether the transformed data is used or the original
  select(!!sym(species), trap, timestamp) %>% 
  pivot_wider(names_from = trap, values_from = !!sym(species), timestamp) %>% 
  setNames(c("timestamp", paste0("trap", c(2, 4, 8))))

corr_matrix <- map(methods, ~corr.test(
  data_corr %>% select(-timestamp),
  use = "complete",
  method = .x,
  adjust = "holm",
  alpha = .05,
  ci = TRUE,
  minlength = 5
  ))

# dummy <- map(corr_matrix, ~.x %>% print(short = FALSE))


```

```{r }
ggthemr("fresh")
ci <- map(corr_matrix, ~.x %>% 
  pluck(10)) %>% 
  bind_rows() %>% 
  round(2) %>% 
  mutate(method = rep(methods, each = 3),
         metric = rep(c("R-", "rho-", "tau-"), each = 3),
         ci = tools::toTitleCase(paste0(metric, method, ": ", lower.adj, " - ", upper.adj)),
         traps = rep(c(24, 28, 48), times = 3),
         x = rep(c(3.5, 3.7, 3.3), each = 3),
         y = rep(c(9, 8.5, 8), each = 3))

gg1 <- data_corr %>% 
  ggplot(aes(x = trap2, y = trap4)) + 
  geom_point(alpha = 0.3) +
  geom_smooth(data = data_corr %>% filter(trap2 != 0), alpha = 0.1) +
  geom_abline(slope = 1, intercept = 0, col = swatch()[4]) +
  geom_label(data = ci %>% filter(traps == 24), aes(label = ci, x = x, y = y), parse = TRUE) +
  coord_cartesian(ylim = c(0, 10), xlim = c(0, 10))

gg2 <- data_corr %>% 
  ggplot(aes(x = trap2, y = trap8)) + 
  geom_point(alpha = 0.3) +
  geom_smooth(data = data_corr %>% filter(trap2 != 0), alpha = 0.1) +
  geom_abline(slope = 1, intercept = 0, col = swatch()[4]) +
  geom_label(data = ci %>% filter(traps == 28), aes(label = ci, x = x, y = y), parse = TRUE) +
  coord_cartesian(ylim = c(0, 10), xlim = c(0, 10))

gg3 <- data_corr %>% 
  ggplot(aes(x = trap4, y = trap8)) + 
  geom_point(alpha = 0.3) +
  geom_smooth(alpha = 0.1) +
  geom_abline(slope = 1, intercept = 0, col = swatch()[4]) +
  geom_label(data = ci %>% filter(traps == 48), aes(label = ci, x = x, y = y), parse = TRUE) +
  coord_cartesian(ylim = c(0, 10), xlim = c(0, 10))

title <- tools::toTitleCase(paste0("Comparison of ", resolution, " average concentrations of ", species, " pollen between the three traps"))

gg_corr <- ggarrange(gg1, gg2, gg3, ncol = 3) %>%
  annotate_figure( top = title, bottom = text_grob("Pairwise correlation between traps; grey line shows the Loess smother; the red line shows a theroratical perfect correlation of 1. \n In the text box one can see the 95% confidence intervals of the R-values (adjusted for multiple comparison) as obtained by Pearson and two robust methods.", color = swatch()[1], face = "italic", size = 10))

gg_corr
```



### Altman-Bland Plots

The well established AB-method for clinical trials can be used here as well to compare the means and differences between traps. Again we see that trap 8 generally has higher measurements and that low concentrations lead to largest differences between the traps. The points lie within the two SD-line for the differences and hence the traps can be assumed to be strongly associated with each other. We also observe larger scattering of the points for lower concentrations.

```{r}

data_altman <- data_transformed %>% 
  select(!!sym(species), trap, timestamp) %>% 
    pivot_wider(names_from = trap, values_from = !!sym(species), timestamp) %>% 
    setNames(c("timestamp", paste0("trap", c(2, 4, 8)))) %>% 
    mutate(mean24 = if_else(!is.na(trap2) | !is.na(trap4),
                          rowSums(.[2:3], na.rm = TRUE) / 2, 
                          NA_real_),
           mean28 = if_else(!is.na(trap2) | !is.na(trap8),
                          rowSums(.[c(2, 4)], na.rm = TRUE) / 2, 
                          NA_real_),
           mean48 = if_else(!is.na(trap4) | !is.na(trap8),
                          rowSums(.[3:4], na.rm = TRUE) / 2, 
                          NA_real_),
           diff24 = trap4 - trap2,
           diff28 = trap8 - trap2,
           diff48 = trap8 - trap4)

sd_diff <- data_altman %>% 
  select(starts_with("diff")) %>% 
  summarise_all(~sd(., na.rm = TRUE)) %>% 
  pivot_longer(1:3, values_to = "sd", names_to = "dummy") %>% 
  pull(sd)

gg_ab1 <- data_altman %>% 
  ggplot(aes(x = mean24, y = diff24)) +
  geom_point(alpha = 0.5) +
  coord_cartesian(xlim = c(min(data_altman$mean24), max(data_altman$mean24)), ylim = c(-sd_diff[1] * 4, sd_diff[1] * 4)) +  
  geom_abline(slope = 0, intercept = 0, col = swatch()[4], alpha = 0.8) +
  geom_abline(slope = 0, intercept = sd_diff[1] * 2, col = swatch()[4], alpha = 0.8, linetype = 3) +
  geom_abline(slope = 0, intercept = sd_diff[1] * (-2), col = swatch()[4], alpha = 0.8, linetype = 3) +
  geom_smooth(alpha = 0.1) +
  labs(y = "Difference(trap4 - trap2)", x = "Mean(trap4, trap2)")
  
gg_ab2 <- data_altman %>% 
  ggplot(aes(x = mean28, y = diff28)) +
  geom_point(alpha = 0.5) +
  coord_cartesian(xlim = c(min(data_altman$mean28), max(data_altman$mean28)), ylim = c(-sd_diff[2] * 4, sd_diff[2] * 4)) +  
  geom_abline(slope = 0, intercept = 0, col = swatch()[4], alpha = 0.8) +
  geom_abline(slope = 0, intercept = sd_diff[2] * 2, col = swatch()[4], alpha = 0.8, linetype = 3) +
  geom_abline(slope = 0, intercept = sd_diff[2] * (-2), col = swatch()[4], alpha = 0.8, linetype = 3) +
  geom_smooth(alpha = 0.1) +
  labs(y = "Difference(trap8 - trap2)", x = "Mean(trap8, trap2)")

gg_ab3 <- data_altman %>% 
  ggplot(aes(x = mean48, y = diff48)) +
  geom_point(alpha = 0.5) +
  coord_cartesian(xlim = c(min(data_altman$mean48), max(data_altman$mean48)), ylim = c(-sd_diff[3] * 4, sd_diff[3] * 4)) +  
  geom_abline(slope = 0, intercept = 0, col = swatch()[4], alpha = 0.8) +
  geom_abline(slope = 0, intercept = sd_diff[3] * 2, col = swatch()[4], alpha = 0.8, linetype = 3) +
  geom_abline(slope = 0, intercept = sd_diff[3] * (-2), col = swatch()[4], alpha = 0.8, linetype = 3) +
  geom_smooth(alpha = 0.1) +
  labs(y = "Difference(trap8 - trap4)", x = "Mean(trap8, trap4)")

title <- "Altman-Bland Plots for all Traps"

gg_ab <- ggarrange(gg_ab1, gg_ab2, gg_ab3, nrow = 3) %>%
  annotate_figure( top = title, bottom = text_grob("Pairwise comparison of traps; grey line shows the Loess smother; the red line shows a theroratical perfect agreement between two traps of zero. \n The dashed red line shows the 2 * sd of the differences, where we expect the points to lie within.", color = swatch()[1], face = "italic", size = 12))

gg_ab
  

```

```{r eval = FALSE}

# This plot is not really useful :-)

data_altman_adj <- data_transformed %>% 
  select(!!sym(species), trap, timestamp) %>% 
  pivot_wider(names_from = trap, values_from = !!sym(species), timestamp) %>% 
  setNames(c("timestamp", paste0("trap", c(2, 4, 8)))) %>% 
  mutate(mean248 = if_else(!is.na(trap2) & !is.na(trap4) & !is.na(trap8),
                        rowSums(.[2:4], na.rm = TRUE) / 3, 
                        NA_real_),
         diff2 = trap2 - mean248,
         diff4 = trap4 - mean248,
         diff8 = trap8 - mean248) %>% 
  pivot_longer(diff2:diff8, names_to = "trap", values_to = "diff")

gg_ab_adj <- data_altman_adj %>% 
  ggplot(aes(x = mean248, y = diff, col = trap)) +
  geom_point(alpha = 0.25) +
  geom_smooth(alpha = 0.1) +
  labs(y = "Difference(Trap_i - Mean)", x = "Mean(trap2, trap4, trap8)") +
  ggtitle("Difference of Individual 2-Hour Measurements from Each Trap\nand the Common Mean")

data_altman_adj_exp <- data_transformed %>% 
  select(!!sym(species), trap, timestamp) %>% 
  pivot_wider(names_from = trap, values_from = !!sym(species), timestamp) %>% 
  setNames(c("timestamp", paste0("trap", c(2, 4, 8)))) %>% 
  mutate(mean248 = if_else(!is.na(trap2) & !is.na(trap4) & !is.na(trap8),
                        rowSums(.[2:4], na.rm = TRUE) / 3, 
                        NA_real_),
         diff2 = trap2 - mean248,
         diff4 = trap4 - mean248,
         diff8 = trap8 - mean248) %>% 
  pivot_longer(diff2:diff8, names_to = "trap", values_to = "diff") %>% 
  mutate(diff = exp(diff),
         mean248 = exp(mean248))

gg_ab_adj_exp <- data_altman_adj %>% 
  ggplot(aes(x = mean248, y = diff, col = trap)) +
  geom_point(alpha = 0.25) +
  geom_smooth(alpha = 0.1) +
  coord_cartesian(ylim = c(0, 4)) +
  labs(y = "Difference(Trap_i - Mean)", x = "Mean(trap2, trap4, trap8)") +
  ggtitle("Difference of Individual 2-Hour Measurements from Each Trap\nand the Common Mean")
  
gg_ab_adj
```


#### For Different Minimum Concentration-Thresholds

This is a quick analysis to visually evaluated the impact of minimum thresholds for the measurements and observe the discrete prioperties of the data.

```{r eval = FALSE}

gg_ab_threshold <- list()

data_ab_threshold <- data_hourly %>% # We work with the highest temporal resolution, to identify the minum concentration that still works
  bind_rows() %>% 
  mutate(trap = as.factor(trap)) %>% 
  group_by(trap, timestamp) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~mean(., na.rm = TRUE)) %>% 
  ungroup 

for (threshold in 0:5){

  data_ab_threshold_filtered <- data_ab_threshold %>% 
    filter(!!sym(species) > threshold*10) %>% 
    mutate_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~log(. + 1))
  
  data_ab_threshold_plot <- data_ab_threshold_filtered %>% 
    select(!!sym(species), trap, timestamp) %>% 
      pivot_wider(names_from = trap, values_from = !!sym(species), timestamp) %>% 
      setNames(c("timestamp", paste0("trap", c(2, 4, 8)))) %>% 
      mutate(mean24 = if_else(!is.na(trap2) | !is.na(trap4),
                            rowSums(.[2:3], na.rm = TRUE) / 2, 
                            NA_real_),
             mean28 = if_else(!is.na(trap2) | !is.na(trap8),
                            rowSums(.[c(2, 4)], na.rm = TRUE) / 2, 
                            NA_real_),
             mean48 = if_else(!is.na(trap4) | !is.na(trap8),
                            rowSums(.[3:4], na.rm = TRUE) / 2, 
                            NA_real_),
             diff24 = trap4 - trap2,
             diff28 = trap8 - trap2,
             diff48 = trap8 - trap4)

  sd_diff <- data_ab_threshold_plot %>% 
    select(starts_with("diff")) %>% 
    summarise_all(~sd(., na.rm = TRUE)) %>% 
    pivot_longer(1:3, values_to = "sd", names_to = "dummy") %>% 
    pull(sd)
  
  gg_ab_threshold[[threshold + 1]] <- data_ab_threshold_plot %>% 
    ggplot(aes(x = mean24, y = diff24)) +
    geom_point(alpha = 0.5) +
    coord_cartesian(xlim = c(0, 8), ylim = c(-sd_diff[1] * 4, sd_diff[1] * 4)) +  
    geom_abline(slope = 0, intercept = 0, col = swatch()[4], alpha = 0.8) +
    geom_abline(slope = 0, intercept = sd_diff[1] * 2, col = swatch()[4], alpha = 0.8, linetype = 3) +
    geom_abline(slope = 0, intercept = sd_diff[1] * (-2), col = swatch()[4], alpha = 0.8, linetype = 3) +
    geom_smooth(alpha = 0.1) +
    labs(y = "Difference(trap4 - trap2)", x = "Mean(trap4, trap2)") +
    geom_label(label = paste("Minimum Concentration: \n", threshold*10, "Pollen/m^3"), aes(x = 1, y = 1.5))
}
  
ggarrange(plotlist = gg_ab_threshold, ncol = 2, nrow = 3) %>%
  annotate_figure( top = "Altman-Bland Plots for Traps 2 and 4 to Compare Hourly Measurements for Different Minimum Concentrations", bottom = text_grob("Comparison of hourly measurements of trap 2 and 4 for different hourly minimum pollen concentrations; \n grey line shows the Loess smother; the red line shows a theroratical perfect agreement between two traps of zero. \n The dashed red line shows the 2 * sd of the differences, where we expect the points to lie within.", color = swatch()[1], face = "italic", size = 12))

```


## Parametric / ANOVA

First we compare the traps with the traditional ANOVA approach. Statistical inference (p-values, confidence intervals, . . . ) is only valid if the model assumptions are fulfilled. So far, this means (many paragraphs are quoted from Lukas Meier ETH - Script Applied Statistics ANOVA Course):

- are the errors independent?
- are the errors normally distributed?
- is the error variance constant?
- do the errors have mean zero?

The first assumption is most crucial (but also most difficult to check). If the independence assumption is
violated, statistical inference can be very inaccurate. In the ANOVA setting, the last assumption is typically
not as important compared to a regression setting, as we are typically fitting “large” models.

Below we prepare the data averaging between the four lines in each trap. As we will see further below in the residuals section, it is necessary to logarithmic the measured concentrations.

### F-Test / Omnibus-Test

We first test for an overall significance of the trap on the measurements.
F = variation between sample means / variation within the samples. I If the P-Value is larger than 5% we don't have to assume that the Null-Hypothesis (measurements from all traps originate from the same distribution).
We use the settings that the coefficients for the three traps add to zero.

```{r}
fit_anova <- aov(as.formula(paste(species, "~ trap")), data = data_transformed, contrasts = c("contr.sum", "contr.poly"))
summary(fit_anova)

```

### Residual Analysis

As mentioned above (and already applied), the residuals of the anova fit need to fulfill some assumptions that we want to check in the following.

- are the errors normally distributed?

In a QQ-plot we plot the empirical quantiles (“what we see in the data”) vs. the theoretical quantiles (“what
we expect from the model”). The plot should show a more or less straight line if the distributional assumption
is correct. By default, a standard normal distribution is the theoretical “reference distribution”.

They are definitely not and we have to do some adjustments. So for the following plot we logarithmic the data to deal with the right-skewedness. The best results were achieved by first logarithmic the data and then taking the square root.

```{r}
fit_anova_raw <- aov(as.formula(paste(species, "~ trap")), data = data_anova, contrasts = c("contr.sum", "contr.poly"))

gg_res1 <- tibble(residuals = residuals(fit_anova_raw, type = "pearson")) %>% 
  ggplot(aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line()

gg_res2 <- tibble(residuals = residuals(fit_anova, type = "pearson")) %>% 
  ggplot(aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line()

ggarrange(gg_res1, gg_res2, nrow = 1) %>% 
  annotate_figure(top = "QQ-Plot for the ANOVA Residuals With (left) and Without Logarithmizing")

```


- is the error variance constant?
- do the errors have mean zero?

The Tukey-Anscombe plot plots the residuals vs. the fitted values. It allows us to check whether the residuals have constant variance and whether the residuals have mean zero (i.e. they don’t show any deterministic pattern). We don't plot the smoothing line as loess (and other) algorithms have issues when the same value is repeated a large number of times (jitter did not really help).

```{r}
# plot(fit_anova, which = 1)

gg_tukey1 <- tibble(resid = residuals(fit_anova_raw, type = "pearson"), fitted = fit_anova_raw$fitted.values) %>%
  ggplot(aes(x = fitted, y = resid)) +
  geom_point(alpha = 0.5, position = position_jitter(width = 5, height = 0)) +
  # geom_smooth(method = "loess", col = swatch()[4]) + # We need to add some jitter, because the same concentrations are repeated a larger number of time: https://stackoverflow.com/questions/38864458/loess-warnings-errors-related-to-span-in-r but it does not really help too much, i will use the base plot for now.
  geom_abline(slope = 0, intercept = 0, col = swatch()[3]) +
  coord_cartesian(ylim = c(-500, 500))

gg_tukey2 <- 
  tibble(resid = residuals(fit_anova, type = "pearson"), fitted = fit_anova$fitted.values) %>%
    ggplot(aes(x = fitted, y = resid)) +
    geom_point(alpha = 0.5, position = position_jitter(width = 0.02, height = 0)) +
    # geom_smooth(method = "loess", col = swatch()[4]) + # We need to add some jitter, because the same concentrations are repeated a larger number of time: https://stackoverflow.com/questions/38864458/loess-warnings-errors-related-to-span-in-r but it does not really help too much, i will use the base plot for now.
    geom_abline(slope = 0, intercept = 0, col = swatch()[3])+
  coord_cartesian(ylim = c(-3, 3))

ggarrange(gg_tukey1, gg_tukey2)  %>% 
  annotate_figure(top = "Tukey Anscombe - Plot for the ANOVA Residuals With (left) and Without Logarithmizing")

```


- are the errors independent?

If the data has some serial structure (i.e., if observations were recorded in a certain time order), we typically
want to check whether residuals close in time are more similar than residuals far apart, as this would be a
violation of the independence assumption. We can do so by using a so-called index plot where we plot the
residuals against time. For positively dependent residuals we would see time periods where most residuals
have the same sign, while for negatively dependent residuals, the residuals would “jump” too often from
positive to negative compared to independent residuals.

```{r}

resid <- residuals(fit_anova_raw, type = "pearson")
resid_df <- tibble(resid = resid, id = as.numeric(names(resid)))

gg_timeline1 <- tibble(id = 1:nrow(data_anova), time = data_anova$timestamp) %>%
  left_join(resid_df, by = "id") %>% 
  ggplot(aes(x = time, y = resid)) +
  geom_point() +
  geom_line(alpha = 0.3)

resid <- residuals(fit_anova, type = "pearson")
resid_df <- tibble(resid = resid, id = as.numeric(names(resid)))

gg_timeline2 <- tibble(id = 1:nrow(data_anova), time = data_anova$timestamp) %>%
  left_join(resid_df, by = "id") %>% 
  ggplot(aes(x = time, y = resid)) +
  geom_point() +
  geom_line(alpha = 0.3)

ggarrange(gg_timeline1, gg_timeline2, nrow = 2) %>% 
  annotate_figure(top = "Index-Plot for the ANOVA Residuals With (left) and Without Logarithmizing")

```

### Contrasts / Tukey HSD

The F-test is rather unspecific. It basically gives us a “Yes/No” answer for the question “is there any
treatment effect at all?”. It doesn’t tell us what specific treatment (or treatment combination) is significant.
Quite often we have a more specific question than the aforementioned global null hypothesis. E.g., we might
want to compare a set of new treatments vs. a control treatment or we want to do pairwise comparisons
between many (or all) treatments.
Multiple Testing: The problem with all statistical tests is the fact that the (overall) error rate increases with increasing number
of tests.

Tukey's test compares the means of every treatment to the means of every other treatment; that is, it applies simultaneously to the set of all pairwise comparisons and identifies any difference between two means that is greater than the expected standard error. In other words, the Tukey method is conservative when there are unequal sample sizes. 
Assumptions:
- The observations being tested are independent within and among the groups.
- The groups associated with each mean in the test are normally distributed.
- There is equal within-group variance across the groups associated with each mean in the test (homogeneity of variance).

https://en.Wikipedia.org/wiki/Tukey%27s_range_test

```{r}
hsd <- TukeyHSD(fit_anova) # This version of Tukey with one-way anova actually shows the differences in the mean
plot(hsd)
```

### Random Effects Model

So far we have been looking at three traps in isolation. But they are part of a larger "population" of Hirst traps being used at MCH or worldwide. Hence we also should look at ANOVA with trap as a random effect. This needs a bit more data per trap, otherwise the model fitting does not reach convergence and produce a so-called singular fit. Parameter estimation for the variance components is typically being done with a technique called restricted maximum likelihood (REML). We could also use “classical” maximum-likelihood estimators here, but REML estimates are less biased. The parameter is estimated with maximum-likelihood assuming that the variances are known.
We want to compare the random effect variance for trap with the Residual variance here.

```{r}
fit_lmer <- lmer(as.formula(paste(species, "~ (1 | trap)")), data = data_transformed)
# isSingular(fit_lmer) # This has to be checked
summary(fit_lmer)
confint(fit_lmer, oldNames = FALSE)
# ranef(fit_lmer)
# confint(fit_anova) # For comparison
```

Checking the residuals again here:

```{r}
par(mfrow = c(1, 2))
qqnorm(ranef(fit_lmer)$trap[, "(Intercept)"], main = "Random effects")
qqnorm(resid(fit_lmer), main = "Residuals")
plot(fit_lmer)
```

## Non-Parametric / Rank-Based Symmetrical

### Kruskal-Wallis Test / Omnibus Test

Kruskal-Wallis test by rank is a non-parametric alternative to one-way ANOVA test, which extends the two-samples Wilcoxon test in the situation where there are more than two groups. It’s recommended when the assumptions of one-way ANOVA test are not met. This tutorial describes how to compute Kruskal-Wallis test in R software. (http://www.sthda.com/english/wiki/kruskal-wallis-test-in-r)

In this case the assumptions are sometimes not met for specific species specially when looking at shorter time windows.

Assumptions

The assumptions of the Kruskal-Wallis test are similar to those for the Wilcoxon-Mann-Whitney test.

- Samples are random samples, or allocation to treatment group is random. 
- The two samples are mutually independent. 
- The measurement scale is at least ordinal, and the variable is continuous. 
- If the test is used as a test of dominance, it has no distributional assumptions. If it used to compare medians, the distributions must be similar apart from their locations. 

The test is generally considered to be robust to ties. However, if ties are present they should not be concentrated together in one part of the distribution (they should have either a normal or uniform distribution)
https://influentialpoints.com/Training/kruskal-wallis_anova-principles-properties-assumptions.htm

The Wilcoxon signed-rank test assumes that the data are distributed symmetrically around the median. In other words, there should be roughly the same number of values above and below the median. This can be checked by visual inspection using histogram or density estimators

```{r}
data_transformed %>% 
  ggplot(aes(x = !!sym(species))) +
  geom_histogram(alpha = 0.7, bins = 50) +
  # geom_freqpoly(col = swatch()[3], bins = 30) +/
  # geom_density(col = swatch()[5]) +
  geom_vline(xintercept = median(data_transformed %>% pull(!!sym(species)), na.rm = TRUE), col = swatch()[4]) +
  ggtitle("Histogram of the Transformed Data With the Median (Red Line)")

```


```{r}
kruskal.test(as.formula(paste(species, "~ trap")), data = data_transformed)
```

### Dunn's test and the Conover-Iman test

The Dunn and Conover Test are typically used as a follow up to the Omnibus test above. It is appropriate to use those test and not the pairwise Wilcoxon Tests for the following two reason: 

- The rank sum test uses different ranks than those employed in the Kruskal-Wallis test (i.e. in both tests you mush the observations together, then rank them, then separate the ranks by group—the rank sum ignores the ranks you got with the omnibus test).
- The rank sum test does not use the pooled variance implied by the null hypothesis in the Kruskal-Wallis test (e.g., just as in one-way ANOVA where the post hoc t tests use an estimate of the pooled variance).

(https://stats.stackexchange.com/questions/71996/differences-between-dwass-steel-critchlow-fligner-and-mann-whitney-u-test-for-a)

The output following the Kruskal-Wallis test provides all possible pairwise comparisons (six in the case of four groups). So the one on the first row compares group B with group A, the first on the second row compares group C with group A, etc.).

The upper number for each comparison is Dunn's pairwise z test statistic. The lower number is in this example the raw p-value associated with the test.
, although this p-value changes depending on the family-wise error rate or false discovery rate multiple comparisons adjustment option. For step wise multiple comparison adjustments (e.g. Holm, Benjamini-Hochberg, etc.), the adjusted p-values will have an asterisk next to them if your would reject the null hypotheses at the specified significance level (which is not necessarily directly indicated by the adjusted p-values since rejection depends on ordering... see the documentation and citations therein for more details.).
https://stats.stackexchange.com/questions/126686/how-to-read-the-results-of-dunns-test

The null hypothesis for each pairwise comparison is that the probability of observing a randomly selected value from the first group that is larger than a randomly selected value from the second group equals one half; this null hypothesis corresponds
to that of the Wilcoxon-Mann-Whitney rank-sum test. Like the ranksum test, if the data can be assumed to be continuous, and the distributions are assumed identical except for a difference in location, Dunn's test may be understood as a test for median difference. 'dunn.test' accounts for tied ranks.

```{r}

dunn.test::dunn.test(data_transformed %>% pull(!!sym(species)),
                     data_transformed$trap,
                     method = "holm") # Try different FEWR /FDR adjustments here

hirst_npar_contrasts <- nparcomp::nparcomp(as.formula(paste(species, "~ trap")), data_transformed, conf.level = 0.95, alternative = "two.sided", type = "Tukey") # ANOVA and regression compare means, while WMW methods calculate the probability that a member of one group will score higher than a member of another group. Being sensitive to differences in the shape of the distribution, which Bruce points out, is not an adverse side-effect. It is the actual purpose of the test, because differences in distribution shape will lead to members of one group out-scoring members of another. https://www.researchgate.net/post/Which_post_hoc_test_is_best_to_use_after_Kruskal_Wallis_test

gg_contrasts <- hirst_npar_contrasts$Analysis %>% 
  ggplot(aes(x = Estimator, y = Comparison)) +
  geom_point() +
  geom_errorbarh(aes(xmax = Upper, xmin = Lower)) +
  geom_label(aes(label = paste("p-Value:", round(p.Value, 2))), nudge_y = 0.2) +
  ggtitle(paste0("Robust Tukey Contrasts for ", tools::toTitleCase(resolution), "-Averages of All Three Traps"))

conover.test::conover.test(data_transformed %>% pull(!!sym(species)),
                     data_transformed$trap,
                     method = "holm")

gg_contrasts
citation("nparcomp")

```




For Donover the ECDF curves are not allow to cross each other. This is actually not given for quite a few species and averaging windows. Maybe it's safer to use Dunn's test (they were usually comparable in terms of the return P-value statistics).

```{r}
data_transformed %>% 
  select(!!sym(species), trap) %>% 
  ggplot(aes(x = !!sym(species), col = trap)) +
  stat_ecdf(geom = "line", pad = FALSE) +
  ggtitle("Empirical Cumulative Distribution Function for All Traps")

```


```{r}
# # This is incorrect, see: https://stats.stackexchange.com/questions/71996/differences-between-dwass-steel-critchlow-fligner-and-mann-whitney-u-test-for-a
# 
# pairwise.wilcox.test(data_anova %>% pull(!!sym(species)), 
#                      data_anova$trap,
#                      paired = FALSE,
#                      p.adjust.method = "fdr",
#                      conf.int = TRUE)
```

### Random Effects Model

When using classic estimation methods, even one such outlier might inflate the between-group variability estimate and distort the results (see example discussed in Section 4). In such a case it would be natural to assume that the group’s random effect (or mean) is an outlier rather than all observations are outliers in the same direction. This concept of allowing potential contamination on different sources of variability leads to the “random effects contamination model”. With this model, we make the assumption of long-tailed or “gross error” distributions for the random effects as well and not just for the random errors. The effect of the contamination is then propagated via the design matrices to the actual observations. Levels of random variability can be hierarchical or crossed, or both, depending on the grouping structure in the data. This implies that the effect of a single outlier on the random effects level is not always as straight forward as in the above mentioned one-way anova example. The effect may be different for each observation as the result of an outlier for a single observation is combined with all the other random effects that affect this observation. This complex relationship between the source of contamination and what is effectively realized in the data can make it very hard or even impossible to spot contamination. This is where robust methods step in and help clear the picture. Basing the robust estimator on the “random effects contamination model” allows not only multiple sources of contamination, it also avoids unnecessary assumptions about the data’s grouping structure. The only assumption on the grouping structure, that is also required for classic estimation, is that the model parameters are estimable. Other contamination models usually assume that contamination is introduced and dealt with at the lowest level only – the level of the observations. In mixed-effects models, observations generally correlate with one another, and robust methods must respect these correlations. These dependencies between observations require other contamination models to make strict assumptions about the grouping structure. The random effects contamination model assumes that contamination occurs directly at the source of random variability, before the grouping structure is introduced, thus circumventing the complexity introduced by the data structure and avoids unnecessary assumptions.

Currently, there are two different methods available for fitting models. They only differ in how the consistency factors for the Design Adaptive Scale estimates are computed. Available fitting methods for theta and sigma. DAStau (default): For this method, the consistency factors are computed using numerical quadrature. This is slower but yields more accurate results. This is the direct analogue to the DAS-estimate in robust linear regression.

```{r}
fit_rlmer <- rlmer(as.formula(paste(species, "~ (1 | trap)")), data = data_transformed)
summary(fit_rlmer)

# # The parameters can be tuned for a more efficient fit, I leave this to the interested reader: 
# fit_rlmer2 <- update(fit_rlmer, rho.sigma.e = psi2propII(smoothPsi, k = 1.5), rho.sigma.b = psi2propII(smoothPsi, k = 1.5))
# compare(fit_lmer, fit_rlmer, fit_rlmer2, show.rho.functions = FALSE)
# gg <- map(1:3, ~plot(fit_rlmer, which = .x))

```

So we can compare the estimated "explained variance" by trap of the total variance for both anova and lmer. For anova this will be calculated further below. For the robust methods there are several options available that are not further discussed here, but can be looked at in these resources:
https://rcompanion.org/handbook/F_08.html
https://www.datanovia.com/en/lessons/kruskal-wallis-test-in-r/
https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full

# Measurement Spread and Error Distribution

A second chapter of the paper should further elaborate on the relative errors between the traps.
We are mostly interested to observe the difference of one specific trap compared to the mean of the three traps at any given point in time. We will compare the relative error for different buckets of pollen concentrations, to see how errors behave when low amounts of pollen are measured. Furthermore, we would like to understand the shape of the error function and if possible fit a parametric curve to the relative differences from the mean.

In this first section we compare relative and absolute differences from the mean. Fitting parameters of a T-Distribution (distribution family visually chosen) using MLE (fitdistr / fit.st). The analysis is based on the transformed values, but the findings are usually independent of the log-transformation.

```{r warning=FALSE}
errors <- data_corr %>% 
  mutate(mean = if_else(!is.na(trap2) | !is.na(trap4) | !is.na(trap8),
                          rowSums(.[2:4], na.rm = TRUE) / 3, 
                          NA_real_)) %>% 
  mutate_at(vars(paste0("trap", c(2, 4, 8))), ~(. - mean)) %>% 
  pivot_longer(cols = trap2:trap8, values_to = "error", names_to = "trap") %>% 
  mutate(error_rel = error / mean) %>% 
  select(-mean)

sd(errors$error, na.rm = TRUE)
sd(errors$error_rel, na.rm = TRUE)

x <- seq(-2.5, 2.5, length.out = 10000)
t_shape <- fitdistr(errors$error[!is.na(errors$error)], "t") # QRM::fit.st(!is.na(errors$error)) #leads to same result without warning

m <- t_shape$estimate[1]
s <- t_shape$estimate[2]
df <- t_shape$estimate[3]
t_errors <- tibble(x = x, y = dt((x - m) / s, df = df) / s)

t_shape_rel <- fitdistr(errors$error_rel[!is.na(errors$error_rel)], "t") # QRM::fit.st(!is.na(errors$error)) #leads to same result without warning
m_rel <- t_shape_rel$estimate[1]
s_rel <- t_shape_rel$estimate[2]
df_rel <- t_shape_rel$estimate[3]
t_errors_rel <- tibble(x = x, y = dt((x - m_rel) / s_rel, df = df_rel) / s_rel)

gg_t_abs <- errors %>%
  ggplot(aes(x=error, y = ..density..)) +
  geom_histogram(bins= 100) +
  geom_density(col = swatch()[4]) +
  geom_rug(aes(y = 0), position = position_jitter(height = 0), col = swatch()[5]) +
  geom_line(data = t_errors, aes(x = x, y = y), col = swatch()[6]) +
  coord_cartesian(xlim = c(-5, 5))

gg_t_rel <- errors %>%
  ggplot(aes(x=error_rel, y = ..density..)) +
  geom_histogram(bins= 100) +
  geom_density(col = swatch()[4]) +
  geom_rug(aes(y = 0), position = position_jitter(height = 0), col = swatch()[5]) +
  geom_line(data = t_errors_rel, aes(x = x, y = y), col = swatch()[6]) +
  coord_cartesian(xlim = c(-2, 2))

ggarrange(gg_t_abs, gg_t_rel, nrow = 2) %>% 
  annotate_figure(top = "Absolute/Relative Differences from the Average Measurements of the Three Traps",
                  bottom = text_grob("On top the absolute differences from the mean fitted with a density kernel estimator (red) and a Student-t distribution (black). \n At the bottom the relative differences from the mean.", color = swatch()[1], face = "italic", size = 10))

```

```{r}

sample <- tibble(x = (x-m)/s, y = pt((x-m)/s, df = df))
sample_rel <- tibble(x = (x-m_rel)/s_rel, y = pt((x-m_rel)/s_rel, df = df))

gg_cdf1 <- errors %>% 
  ggplot(aes(x = error)) +
  stat_ecdf(geom = "step", pad = FALSE) +
  coord_cartesian(xlim = c(-2.5, 2.5)) +
  geom_line(data = sample, aes(x = x, y = y))

gg_cdf2 <- errors %>% 
  ggplot(aes(x = error_rel)) +
  stat_ecdf(geom = "step", pad = FALSE) +
  coord_cartesian(xlim = c(-0.5, 0.5)) +
  geom_line(data = sample_rel, aes(x = x, y = y))

ggarrange(gg_cdf1, gg_cdf2, nrow = 1) %>%
  annotate_figure(top = "Comparison of Empirical and Fitted CDF for Absolute (Left) and Relative Differences")
```
  
Now we can investigate the goodness of the fit of the selected t-Distribution above.
Graphically the QQ-Plot versus a t-distribution allows us to check whether the choice of distribution-family was sensible.

```{r}
errors %>% 
  mutate(error_scaled = (error)) %>%
  ggplot(aes(sample = error_scaled)) +
  stat_qq(distribution = qt, dparams = as.list(df)) +
  stat_qq_line(distribution = qt, dparams = as.list(df)) +
  ggtitle("QQ-Plot to Compare Empirical Quantiles with Theoretical Ones from a t-Distribution")
```

Then we can carry out two statistical tests that measure the goodness-of-fit.
Parameters have been estimated from data, so the tests in the following need to account for that (e.g. no KS-tests).

The Anderson–Darling and Cramér–von Mises statistics belong to the class of quadratic EDF statistics (tests based on the empirical distribution function. The Anderson–Darling test assesses whether a sample comes from a specified distribution. It makes use of the fact that, when given a hypothesized underlying distribution and assuming the data does arise from this distribution, the cumulative distribution function (CDF) of the data can be assumed to follow a uniform distribution. The data can be then tested for uniformity with a distance test (Shapiro 1980). [Wikipedia]

```{r}
cvm.test((errors$error[!is.na(errors$error)] - m)/s, "pt", df = df, estimated = TRUE)
cvm.test((errors$error_rel[!is.na(errors$error_rel)] - m_rel)/s_rel, "pt", df = df_rel, estimated = TRUE)

ad.test((errors$error[!is.na(errors$error)] - m)/s, "pt", df = df, estimated = TRUE)
ad.test((errors$error_rel[!is.na(errors$error_rel)] - m_rel)/s_rel, "pt", df = df_rel, estimated = TRUE)

```


## Relative Errors

Looking at absolute errors in log scale (as we did above) is closely related to relative differences on an absolute scale. This is what we are doing from now on out. 

```{r}
if (resolution == "daily"){
  data_errors_raw <- data_daily_raw
} else if (resolution == "12hour"){
  data_errors_raw <- data_hours12_raw
} else if (resolution == "6hour"){
  data_errors_raw <- data_hours6_raw
} else if (resolution == "3hour"){
  data_errors_raw <- data_hours3_raw
} else if (resolution == "2hour"){
  data_errors_raw <- data_hours2_raw
} else if (resolution == "hourly"){
  data_errors_raw <- data_hourly_raw
}

data_errors_raw <- data_errors_raw %>% 
  bind_rows() %>% 
  mutate(trap = as.factor(trap),
         hour = paste0(sprintf("%02d", hour), ":00"),
         timestamp = ymd_hm(paste0(as.character(date), hour))) %>% 
  group_by(trap, timestamp) %>% 
  summarise_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), ~mean(., na.rm = TRUE)) %>% 
  ungroup  %>% 
  mutate(type = "dummy",
         hour = hour(timestamp),
         date = date(timestamp))

# This is part of the code of the set_na() function in this package. slightly adjusted for this specific setup
measurements_to_exclude <- data_errors_raw %>%
    group_by(trap, timestamp) %>%
    replace(is.na(.), 0) %>% # This should do nothing :-)
    mutate_at(vars(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), .funs = list(exclude = ~mean(.))) %>%
    ungroup() %>%
    group_by(timestamp) %>%
    mutate_at(vars(all_of(paste0(c(species_selection, "Total", "Group1", "Group2", "Group3", "Group4", "Group5"), "_exclude"))), function(x){
      if(any(as.integer(x) == 0))
        return(NA) # This will set all measurements to NA if no trap measured any pollen
      else
        return(0)
    }) %>%
    ungroup()

measurements_after_exclusion <- map2_df(data_errors_raw %>% select(all_of(species_selection), Total, Group1, Group2, Group3, Group4, Group5), 
                                        measurements_to_exclude %>% select(contains("exclude")), 
                                        ~.x + as.integer(.y))

data_errors <- measurements_after_exclusion %>%
    bind_cols(measurements_to_exclude %>%
                select(timestamp, date, hour, trap, type))

```

### Concentration Groups

For the concentrations we are looking at some density plots and histograms to start with and then we look at the relative differences from the common mean of the three traps.

```{r }
data_errors_conc_raw <- data_errors_raw %>% 
  select(-type) %>%
  pivot_longer(Castanea:Picea, names_to = "type", values_to = "value") %>% 
  select(trap, timestamp, type, value) %>% 
  pivot_wider(names_from = trap) %>% 
  setNames(c("timestamp", "type", "trap2", "trap4", "trap8")) %>% 
  mutate(measuring_traps = as.integer(!is.na(trap2)) + as.integer(!is.na(trap4)) + as.integer(!is.na(trap8)),
         mean = if_else(!is.na(trap2) | !is.na(trap4) | !is.na(trap8), # For the density plots we want to see all observations, even if the mean was calculated based on only one trap.
                          rowSums(.[3:5], na.rm = TRUE) / measuring_traps, 
                          NA_real_)) %>% 
  pivot_longer(trap2:trap8, names_to = "trap") %>% 
  mutate(conc = case_when(
    mean >= 0 & mean < 10 ~ "Group00_10",
    mean >= 10 & mean < 20 ~ "Group10_20",
    mean >= 20 & mean < 50 ~ "Group20_50",
    mean >= 50 & mean < 100 ~ "Group50_100",
    mean >= 100 & mean < 300 ~ "Group100_300",
    mean >= 300 ~ "Group300"
  )) %>% 
  select(timestamp, trap, value, conc, type) %>% 
  pivot_wider(names_from = conc, values_from = value)

data_errors_conc <- data_errors %>% 
  select(-type) %>%
  pivot_longer(Castanea:Picea, names_to = "type", values_to = "value") %>% 
  select(trap, timestamp, type, value) %>% 
  pivot_wider(names_from = trap) %>% 
  setNames(c("timestamp", "type", "trap2", "trap4", "trap8")) %>% 
  mutate(measuring_traps = as.integer(!is.na(trap2)) + as.integer(!is.na(trap4)) + as.integer(!is.na(trap8)),
         mean = if_else(!is.na(trap2) & !is.na(trap4) & !is.na(trap8), # For the error fitting exercise we only look at timestamps where all three traps measured. This is again defined in the respective functions in used further below in the sampling section.
                          rowSums(.[3:5], na.rm = TRUE) / measuring_traps, 
                          NA_real_)) %>% 
  pivot_longer(trap2:trap8, names_to = "trap") %>% 
  mutate(conc = case_when(
    mean >= 0 & mean < 10 ~ "Group00_10",
    mean >= 10 & mean < 20 ~ "Group10_20",
    mean >= 20 & mean < 50 ~ "Group20_50",
    mean >= 50 & mean < 100 ~ "Group50_100",
    mean >= 100 & mean < 300 ~ "Group100_300",
    mean >= 300 ~ "Group300"
  )) %>% 
  select(timestamp, trap, value, conc, type) %>% 
  pivot_wider(names_from = conc, values_from = value) %>% 
  select(-"NA")



```

```{r eval = FALSE}
gg_conc_dens <- list()
gg_conc_hist <- list()

labels_y <- list(1, 0.065, 0.025, 0.015, 0.004, 0.0015)
labels_y <- list(0.75, 0.03, 0.015, 0.010, 0.005, 0.0012) # For 2-hours
labels_y_hist <- list(15, 13, 10, 10, 7.5, 3)
names(labels_y) <- c("Group00_10", "Group10_20", "Group20_50", "Group50_100", "Group100_300", "Group300")
names(labels_y_hist) <- c("Group00_10", "Group10_20", "Group20_50", "Group50_100", "Group100_300", "Group300")

for (j in c("Group00_10", "Group10_20", "Group20_50", "Group50_100", "Group100_300", "Group300")){
  obs <- data_errors_conc_raw %>% 
    filter(!is.na(!!sym(j))) %>% 
    summarise(n()/3) %>% 
    pull 
  obs <- paste("# of Observations:", obs)
  
  gg_conc_dens[[j]] <- data_errors_conc_raw %>% 
    filter(!is.na(!!sym(j)))  %>% 
    ggplot() +
    geom_density(aes(x = !!sym(j), col = trap, fill = trap), alpha = 0.15) + # The area under that whole curve should be 1. To get an estimate of the probability of certain values, you'd have to integrate over an interval on your 'y' axis, and that value should never be greater than 1.
    geom_label(label = obs, aes(x = max(!!sym(j)) * 0.7), y = labels_y[[j]]) +
    coord_cartesian(xlim = c(0, NA))
}

gg_dens_conc <- ggarrange(plotlist = gg_conc_dens, nrow = 3, ncol = 2) %>% 
  annotate_figure(top = paste("Comparison of", tools::toTitleCase(resolution), "Measurements of the Three Traps for all Species and Different Concentration Groups."),
                  bottom = text_grob("We are looking at Density Kernel Estimators for all three traps to compare the measurements between them. \n The area under each curve adds up to 1 and makes it possible to vizualise the (dis-)similarities of measurements from the three traps. It is basically a smoothed histogram.", color = swatch()[1], face = "italic", size = 10))

# gg_dens_conc
```

```{r eval = FALSE}
for (j in c("Group00_10", "Group10_20", "Group20_50", "Group50_100", "Group100_300", "Group300")){
  obs <- data_errors_conc_raw %>% 
    filter(!is.na(!!sym(j))) %>% 
    summarise(n()/3) %>% 
    pull 

  obs <- paste("# of Observations:", obs)
  
  gg_conc_hist[[j]] <- data_errors_conc_raw %>% 
    filter(!is.na(!!sym(j)))  %>% 
    ggplot() +
    geom_histogram(data = subset(data_errors_conc_raw, trap == "trap4"), aes(x = !!sym(j)), col = swatch()[3], fill = swatch()[3], alpha = 0.06, bins = 20) + 
    geom_histogram(data = subset(data_errors_conc_raw, trap == "trap8"), aes(x = !!sym(j)), col = swatch()[4], fill = swatch()[4], alpha = 0.06, bins = 20) + 
    geom_histogram(data = subset(data_errors_conc_raw, trap == "trap2"), aes(x = !!sym(j)), col = swatch()[2], fill = swatch()[2], alpha = 0.06, bins = 20)  + 
    geom_label(label = obs, aes(x = max(!!sym(j)) * 0.8), y = labels_y_hist[[j]])
}

ggarrange(plotlist = gg_conc_hist, nrow = 3, ncol = 2) %>% 
  annotate_figure(top = "Comparison of Measurements of the Three Traps for all Pollen-Species and Different Concentration Groups.",
                  bottom = text_grob("We are looking at Counts for all three traps to compare the measurements between them. Trap2 in blue, Trap4 in grey, Trap8 in red.", color = swatch()[1], face = "italic", size = 10))

```

```{r fig.height=7, fig.width=10}
gg_hist_conc <- list()

if (resolution == "2hour"){
  conc_y_label <- c(2.8, 1.1, 0.75, 1, 1.2, 1.5) 
} else {
  conc_y_label <- c(1, 1.4, 1.6, 1.8, 2, 2.5) # This has to be adjusted depending on the temporal averaging window. You could probably find a smart rule, I just did not find it :-)
}

names(conc_y_label) <- c("Group00_10", "Group10_20", "Group20_50", "Group50_100", "Group100_300", "Group300")

for (j in c("Group00_10", "Group10_20", "Group20_50", "Group50_100", "Group100_300", "Group300")){
  gg_hist_conc[[j]] <- plot_hist_dt(data_errors_conc, j, n_test = 1, samples = 1, plots = TRUE, tdist = FALSE, y_label = conc_y_label[j])
}

gg_error_conc <- ggarrange(plotlist = gg_hist_conc, nrow = 3, ncol = 2) %>% 
  annotate_figure(top = paste("Ratios of Individual", tools::toTitleCase(resolution), "Measurements of the Three Traps vs Average Measurements for Six Pollen Concentration Groups"), bottom = text_grob("We are looking at measurement ratios fitted with a density kernel estimator (red).", color = swatch()[1], face = "italic", size = 10))
```

These plots are very nice to observe the variance in the data, as they allow to also compare times where only part of the traps measured. 

```{r fig.height=7, fig.width=10}

data_conc_diff <- data_errors_conc_raw %>% 
  # filter(type == species) %>% # Feel free to filter by species, this might not be the best idead though, as the number of observations becomes quite small.
  pivot_longer(4:9, names_to = "group", values_to = "value") %>% 
  pivot_wider(names_from = trap) %>% 
  mutate(diff24 = abs((trap2-trap4) / ((trap2+trap4)/2)),
         diff28 = abs((trap2-trap8) / ((trap2+trap8)/2)),
         diff48 = abs((trap4-trap8) / ((trap4+trap8)/2))) %>% 
  ungroup 

means <- data_conc_diff %>% 
  pivot_longer(diff24:diff48, names_to = "Traps") %>% 
  mutate(Traps = factor(Traps, labels = c("2 - 4", "2 - 8", "4 - 8"))) %>%
  group_by(group, Traps) %>% 
  summarise(mean = mean(value, na.rm = TRUE))

gg_boxplot_reldiff <- data_conc_diff %>% 
  pivot_longer(diff24:diff48, names_to = "Traps") %>% 
  mutate(group = factor(group, levels = c("Group00_10", "Group10_20", "Group20_50", "Group50_100", "Group100_300", "Group300")),
         Traps = factor(Traps, labels = c("2 - 4", "2 - 8", "4 - 8"))) %>% 
  ggplot(aes(x = group, y = value, fill = Traps)) +
  geom_boxplot(alpha = 0.6) +
  geom_point(data = means, aes(x = group, y = mean, col = Traps), position = position_dodge(width = 0.75), shape = 95, size = 10, show.legend = FALSE) +
  labs(x = "", y = "abs(trapᵢ - trapⱼ) / mean(trapᵢ, trapⱼ)", title = paste("Relative Pairwise Differences for Daily Average Concentrations of", species, "Pollen")) +
  scale_y_continuous(labels = scales::percent)

```

### Species

```{r fig.height=7, fig.width=10}
gg_hist_species <- list()

conc_y_label_species <- c(3, 2.4, 1.6, 1.5, 2, 1.4)  # This has to be adjusted depending on the temporal averaging window. You could probably find a smart rule, I just did not find it :-)
conc_y_label_species <- c(1.3, 1, 0.9, 0.8, 0.7, 0.8) # 2-hour
names(conc_y_label_species) <- c("Fraxinus", "Poaceae", "Betula", "Quercus", "Pinus", "Cupressus")

for (j in c("Fraxinus", "Poaceae", "Betula", "Quercus", "Pinus", "Cupressus")){
  gg_hist_species[[j]] <- plot_hist_dt(data_errors, j, n_test = 1, samples = 1, plots = TRUE, tdist = FALSE, y_label = conc_y_label_species[j])
}
gg_error_species <- ggarrange(plotlist = gg_hist_species, nrow = 3, ncol = 2) %>% 
  annotate_figure(top = paste("Ratios of Individual", tools::toTitleCase(resolution), "Measurements of the Three Traps vs Average Measurements for Top-6 Species"), bottom = text_grob("We are looking at measurement ratios fitted with a density kernel estimator (red).", color = swatch()[1], face = "italic", size = 10))
```



### Pollen Grain Size Groups

```{r fig.height=7, fig.width=10}

# I manually removed the fitted t-distributions stuff from the plot_hist_dt function for this and then put it back in afterwards.

conc_y_label_size <- c(1.9, 1.8, 2.4, 1.3, 2, 2.5)  # This has to be adjusted depending on the temporal averaging window. You could probably find a smart rule, I just did not find it :-)
conc_y_label_size <- c(1, 1, 0.9, 0.8, 0.9, 0.9) # 2-hour
names(conc_y_label_size) <- c("Group1", "Group2", "Group3", "Group4", "Group5", "Total")

gg_hist_groups <- list()

for (j in c("Group1", "Group2", "Group3", "Group4", "Group5", "Total")){
  gg_hist_groups[[j]] <- plot_hist_dt(data_errors, species = j, n_test = 1, samples = 1, plots = TRUE, tdist = FALSE, y_label = conc_y_label_size[j])
}
gg_error_size <- ggarrange(plotlist = gg_hist_groups, nrow = 3, ncol = 2) %>% 
  annotate_figure(top = paste("Ratios of Individual", tools::toTitleCase(resolution), "Measurements of the Three Traps vs Average Measurements for Five Pollen Size Groups"), bottom = text_grob("We are looking at measurement ratios fitted with a density kernel estimator (red).", color = swatch()[1], face = "italic", size = 10))


```

# Sample Blooming Season Starts and SPI with Theoretical Error Distribution

This is investigatoy work in progress. We will take the mean values from the three traps above and then create bootstrap samples for the divergence from the mean. Based on these samples we will investigate the spread of the SPI and the Start of the Blooming Season; two metrics highly relevant for Pollen prediction. This analysis will be carried out on the most robust (i.e. daily-averages) values.

Some of the following metrics only make sense with daily averages. Hence the daily data is hardcoded in this chapter. The sampling will only be carried out with daily values (as discussed with Regula Gehrig).

## For Different Species

```{r eval = FALSE}
# set.seed(123)
data_sampling <- data_errors

data_sampling_raw <- data_errors_raw

gg_hist_species <- list()
samples_species <- list()

for (j in c("Fraxinus", "Poaceae", "Betula", "Quercus", "Pinus", "Cupressus")){
  samples_species[[j]] <- plot_hist_dt(data_sampling, j, n_test = 100, samples = 1000)
  gg_hist_species[[j]] <- plot_hist_dt(data_sampling, j, n_test = 100, samples = 1000, plots = TRUE)
}

ggarrange(plotlist = gg_hist_species, nrow = 3, ncol = 2) %>% 
  annotate_figure(top = "Ratios of Individual Measurements of the Three Traps vs Average Measurements for Top-6 Species",
                  bottom = text_grob("We are looking at measurement ratios fitted with a density kernel estimator (red) and a Student-t distribution (black). \n In the left box we see the bootstrapped P-value for the two goodness-of-fit tests. In the right box we see the sd and degrees of freedom of the fitted t-Distribution.", color = swatch()[1], face = "italic", size = 10))

```


```{r eval = FALSE}

# The sampled errors from above are differences of each trap from the common mean (logarithmic).
# To evaluate the robustness of our metrics we should therefore convert them back into absolute differences from the common mean in Pollen per m^3.

gg_spi_species <- list()

for (j in c("Fraxinus", "Poaceae", "Betula", "Quercus", "Pinus", "Cupressus")){
 gg_spi_species[[j]] <- plot_spi(data_sampling_raw, species = j, samples = samples_species, n = 1000, xlim = c(0, 1.2e4))
}

ggarrange(plotlist = gg_spi_species, ncol = 1) %>% 
  annotate_figure(top = "Boxplot of Sampled SPI for Top-6 Species",
                  bottom = text_grob("The SPI was calculated for 1000 random samples for the top 6 species. Above we can see the boxplot of the resulting SPI-samples.", color = swatch()[1], face = "italic", size = 10))

```


```{r eval = FALSE}

days <- data_sampling_raw %>% select(timestamp) %>% pull %>% unique

season_start <- list()
season_start_all <- list()
gg_season_start <- list()

for (j in c("Fraxinus", "Poaceae", "Betula", "Quercus", "Pinus", "Cupressus")){ 

  if (length(samples_species[[j]]) > 1) {
      mean_traps <- data_sampling_raw %>%
        select(!!sym(j), trap, timestamp) %>%
        pivot_wider(names_from = trap, values_from = !!sym(j), timestamp) %>%
        setNames(c("timestamp", paste0("trap", c(2, 4, 8)))) %>%
        mutate(measuring_traps = as.integer(!is.na(trap2)) + as.integer(!is.na(trap4)) + as.integer(!is.na(trap8)),
               mean = if_else(!is.na(trap2) | !is.na(trap4) | !is.na(trap8),
                              rowSums(.[2:4], na.rm = TRUE) / measuring_traps,
                              NA_real_)) %>%
        pull(mean) %>%
        na.omit()
    
      sampled_days <- map(mean_traps, ~ .x * pmax(0, samples_species[[j]]))
      
      for (i in 1:100){
        
        sampled_days_random <- map(sampled_days, ~sample(.x, 1)) %>% unlist()
  
        if (j == "Poaceae"){
          season_start[[i]] <- tibble(date = as.Date(as.Date("2013-04-03"):as.Date("2013-06-30"), origin = "1970-01-01"), !!j := sampled_days_random) %>% 
            AeRobiology::calculate_ps(method = "clinical", plot = FALSE, th.pollen = 3, th.sum = 30) %>% #https://onlinelibrary.wiley.com/doi/full/10.1111/all.13092
            pull(st.dt)
        } else {
          season_start[[i]] <- tibble(date = as.Date(as.Date("2013-04-03"):as.Date("2013-06-30"), origin = "1970-01-01"), !!j := sampled_days_random) %>% 
            AeRobiology::calculate_ps(method = "clinical", plot = FALSE, th.pollen = 10, th.sum = 100) %>% 
            pull(st.dt)
        }
    }
  season_start_all[[j]] <- do.call("c", season_start)
  gg_season_start[[j]] <- tibble(date = substr(as.character(season_start_all[[j]]), 6, 10)) %>% 
    ggplot(aes(x = date)) +
    labs(x = j) +
    geom_histogram(stat = "count")
  }
}

ggarrange(plotlist = gg_season_start[-6], ncol = 2, nrow = 3) %>% 
  annotate_figure(top = "Histrograms of Sampled Starts of Blooming Season for Top-5 Species",
                  bottom = text_grob("The start was calculated for 1000 random samples for the top 5 species. \n The samples were generated with the t-distribution fitted to the relative differences of one trap from the mean measurements.", color = swatch()[1], face = "italic", size = 10))

```


## For Different Pollen Size Groups

```{r eval = FALSE}
# set.seed(123)

ggthemr("fresh")

gg_hist_groups <- list()
samples_groups <- list()

for (j in c("Group1", "Group2", "Group3", "Group4", "Group5", "Total")){
  samples_groups[[j]] <- plot_hist_dt(data_sampling, species = j, n_test = 100, samples = 1000)
  gg_hist_groups[[j]] <- plot_hist_dt(data_sampling, species = j, n_test = 100, samples = 1000, plots = TRUE)
}


ggarrange(plotlist = gg_hist_groups, nrow = 3, ncol = 2) %>% 
  annotate_figure(top = "Ratios of Individual Measurements of the Three Traps vs Average Measurements for Pollen Size Groups",
                  bottom = text_grob("We are looking at measurement ratios fitted with a density kernel estimator (red) and a Student-t distribution (black). \n In the left box we see the bootstrapped P-value for the two goodness-of-fit tests. In the right box we see the sd and degrees of freedom of the fitted t-Distribution.", color = swatch()[1], face = "italic", size = 10))

```

```{r eval = FALSE}

# The sampled errors from above are differences of each trap from the common mean (logarithmic).
# To evaluate the robustness of our metrics we should therefore convert them back into absolute differences from the common mean in Pollen per m^3.

gg_spi_groups <- list()


for (j in c("Group1", "Group2", "Group3", "Group4", "Group5", "Total")){
 gg_spi_groups[[j]] <- plot_spi(data_sampling_raw, j, samples = samples_groups, n = 1000, xlim = c(0, 3.5e4))
}

ggarrange(plotlist = gg_spi_groups, ncol = 1) %>% 
annotate_figure(top = "Boxplot of Sampled SPI for Pollen Size Groups",
                  bottom = text_grob("The SPI was calculated for 1000 random samples for Pollen Size Groups. Above we can see the boxplot of the resulting SPI-samples.", color = swatch()[1], face = "italic", size = 10))


```



## For Different Pollen-Concentrations


```{r eval = FALSE}
# set.seed(123)

ggthemr("fresh")

gg_hist_conc <- list()
samples_conc <- list()

for (j in c("Group00_10", "Group10_20", "Group20_50", "Group50_100", "Group100_300", "Group300")){
  samples_conc[[j]] <- plot_hist_dt(data_sampling_conc, j, n_test = 1, samples = 10)
  gg_hist_conc[[j]] <- plot_hist_dt(data_sampling_conc, j, n_test = 1, samples = 10, plots = TRUE)
}

ggarrange(plotlist = gg_hist_conc, nrow = 3, ncol = 2) %>% 
  annotate_figure(top = "Ratios of Individual Measurements of the Three Traps vs Average Measurements for Pollen Concentration Groups",
                  bottom = text_grob("We are looking at measurement ratios fitted with a density kernel estimator (red) and a Student-t distribution (black). \n In the left box we see the bootstrapped P-value for the two goodness-of-fit tests. In the right box we see the sd and degrees of freedom of the fitted t-Distribution.", color = swatch()[1], face = "italic", size = 10))
```

```{r eval = FALSE}

# The sampled errors from above are differences of each trap from the common mean (logarithmic).
# To evaluate the robustness of our metrics we should therefore convert them back into absolute differences from the common mean in Pollen per m^3.
ggthemr("fresh")
gg_spi_conc <- list()


for (j in c("Group00_10", "Group10_20", "Group20_50", "Group50_100", "Group100_300", "Group300")){
 gg_spi_conc[[j]] <- plot_spi(data_sampling_conc_raw, j, samples = samples_conc, n = 10, xlim = c(0, 3.5e4))
}

ggarrange(plotlist = gg_spi_conc, ncol = 1) %>% 
annotate_figure(top = "Boxplot of Sampled SPI for Mean-Concentration Groups",
                  bottom = text_grob("The SPI was calculated for 1000 random samples for Mean-Concentration-Groups. Above we can see the boxplot of the resulting SPI-samples.", color = swatch()[1], face = "italic", size = 10))


```

# Some Thoughts

A short review of the methods applied in the draft paper so far. I would like to lay out why the methods used above are probably a preferred choice to compare the traps.

## Correlation vs. Altman-Bland

The Correlation coefficient r is not a reliable metric to compare measurers! see above

## Paired vs. Unpaired

I am pretty sure that paired tests should only be applied if the measurements come from the same trap (i.e. between lines). This is not the case for the trap comparison though!
Paired t-test compares study subjects at 2 different times (paired observations of the same subject). Unpaired t-test (aka Student’s test) compares two different subjects. The paired t-test reduces intersubject variability (because it makes comparisons between the same subject), and thus is theoretically more powerful than the unpaired t-test.

## Pairwise vs. Contrasts

Problem of multiple testing and adjustments of P-Value, the chances of making type one error (reject H0 although true) is actually higher then the alpha chosen.

## Non-parametric vs. parametric

Assumption of linearity is harder to achieve without adding much benefit (Wilcox vs. t-test for simple tests.). It probably makes sense to test apply some robust test in addition to the standard anova t-test that rely on normal distributions.

## linear regression vs. ordinary least products regression analysis 

Geometric mean regression, reduced major axis regression take into account that both y and x contain measurement errors. If measurements have been made on a continuous scale, the main choice is between the Altman–Bland method of differences and least products regression analysis. It is argued that although the former is relatively simple to execute, it does not distinguish adequately between fixed and proportional bias. Least products regression analysis, although more difficult to execute, does achieve this goal. There is almost universal agreement among bio-statisticians that the Pearson product–moment correlation coefficient (r) is valueless as a test for bias.
http://www.jerrydallal.com/LHSP/compare.htm

# Plots for Paper

In the following I create some additional tables for the paper and in the end export all the relevant figures and tables. This section could definitely need a general overhaul, it is quite lengthy right now... There are very little comments added to this section, I would refer the interested reader to the paper that was written based on this analysis.

## Contrasts

### Single Species

```{r}

fit_anova_contr <- list()
hsd_contr <- list()
hsd_per_res <- list()
npar_contr <- list()
npar_contr_res <- list()
npar_contr_overall <- list()
kruskal_contr <- list()

species_contr <- c("Total", "Fraxinus", "Poaceae", "Betula", "Quercus", "Pinus")

# Parametric

for (i in species_contr){
  fit_anova_contr[[i]] <- map(data_contr_tf, ~aov(as.formula(paste(i, "~ trap")), data = .x, contrasts = c("contr.sum", "contr.poly")))  
  hsd_contr[[i]] <- map(fit_anova_contr[[i]], ~TukeyHSD(.x))
  hsd_per_res[[i]] <- map(hsd_contr[[i]], ~.x[["trap"]])
  
  npar_contr[[i]] <- map(data_contr_tf, ~nparcomp::nparcomp(as.formula(paste(i, "~ trap")), data = .x, conf.level = 0.95, alternative = "two.sided", type = "Tukey"))
  npar_contr_res[[i]] <- map(npar_contr[[i]], ~.x[["Analysis"]])
  npar_contr_overall[[i]] <- map(npar_contr[[i]], ~.x[["Overall"]])

}

number_obs_contr <- map(data_contr_tf, ~map(.x, ~length(.x[!is.na(.x)]) / 3))

species_contr_sel <- "Fraxinus"
title_contr <- paste(species_contr_sel, "Pollen Concentrations")
header_contr <- c(title_contr = 5)
names(header_contr) <- title_contr

hsd_per_res %>% 
  extract2(species_contr_sel) %>% # Specify Resolution Here
  map( ~ .x %>% as_tibble) %>% 
  bind_rows() %>% 
  mutate_all(~round(., 3)) %>% 
  mutate(res = rep(names(data_aggs), each = 3),
         traps = rep(c("4-2", "8-2", "8-4"), times = 5)) %>% 
  select(Traps = traps, "Mean-Difference" = diff, "Lower" = lwr, "Upper" = upr, "p-Value" = "p adj") %>% 
  kable() %>% 
  kable_styling("striped", full_width = FALSE) %>% 
    pack_rows(start_row = 1, end_row = 3, group_label = paste("2-Hour Averages, Observations:", number_obs_contr$data_hours2[[species_contr_sel]])) %>% 
    pack_rows(start_row = 4, end_row = 6, group_label = paste("3-Hour Averages, Observations:", number_obs_contr$data_hours3[[species_contr_sel]])) %>% 
    pack_rows(start_row = 7, end_row = 9, group_label = paste("6-Hour Averages, Observations:", number_obs_contr$data_hours6[[species_contr_sel]])) %>% 
    pack_rows(start_row = 10, end_row = 12, group_label = paste("12-Hour Averages, Observations:", number_obs_contr$data_hours12[[species_contr_sel]])) %>% 
    pack_rows(start_row = 13, end_row = 15, group_label = paste("Daily Averages, Observations:", number_obs_contr$data_daily[[species_contr_sel]])) %>% 
  add_header_above(header_contr)


npar_contr_res %>%
  extract2(species_contr_sel) %>% # Specify Resolution Here
  bind_rows() %>%
  mutate_if(is.numeric, round, 3) %>%
  mutate(res = rep(names(data_aggs), each = 3)) %>%
  select(Traps = Comparison, Estimator, Lower, Upper, "p-Value" = "p.Value") %>%
  kable() %>%
  kable_styling("striped", full_width = FALSE) %>% 
    pack_rows(start_row = 1, end_row = 3, group_label = paste("2-Hour Averages, Observations:", number_obs_contr$data_hours2[[species_contr_sel]])) %>% 
    pack_rows(start_row = 4, end_row = 6, group_label = paste("3-Hour Averages, Observations:", number_obs_contr$data_hours3[[species_contr_sel]])) %>% 
    pack_rows(start_row = 7, end_row = 9, group_label = paste("6-Hour Averages, Observations:", number_obs_contr$data_hours6[[species_contr_sel]])) %>% 
    pack_rows(start_row = 10, end_row = 12, group_label = paste("12-Hour Averages, Observations:", number_obs_contr$data_hours12[[species_contr_sel]])) %>% 
    pack_rows(start_row = 13, end_row = 15, group_label = paste("Daily Averages, Observations:", number_obs_contr$data_daily[[species_contr_sel]])) %>% 
  add_header_above(header_contr)

```

### Top-5 Species - Anova

```{r}
two_hour_contr <- map(hsd_per_res, ~.x %>% extract(c(1))) 
daily_contr <- map(hsd_per_res, ~.x %>% extract(c(5))) 

two_hour_table <- two_hour_contr %>% 
  unlist(recursive = FALSE) %>% 
  map(~ .x %>% as_tibble) %>% 
  bind_rows() %>% 
  mutate_if(is.numeric, round, 3) %>%
  mutate(traps = rep(c("4-2", "8-2", "8-4"), times = 6)) %>% 
  select(Traps = traps, "Mean-Difference" = diff, "Lower" = lwr, "Upper" = upr, "p-Value" = "p adj")

daily_table <- daily_contr %>% 
  unlist(recursive = FALSE) %>% 
  map(~ .x %>% as_tibble) %>% 
  bind_rows() %>% 
  mutate_if(is.numeric, round, 3) %>%
  mutate(traps = rep(c("4-2", "8-2", "8-4"), times = 6)) %>% 
  select("Mean-Difference" = diff, "Lower" = lwr, "Upper" = upr, "p-Value" = "p adj")

kable_contr_top <- two_hour_table %>% 
  mutate(`p-Value` = if_else(`p-Value` < 0.05, 
                             cell_spec(`p-Value`, color = "red"),
                             cell_spec(`p-Value`))) %>% 
  bind_cols(daily_table) %>% 
  mutate(species = rep(species_contr, each = 3),
         species = cell_spec(species, angle = 270, align =  "center")) %>% 
  select(species, everything()) %>% 
  setNames(c("Species", names(two_hour_table), names(two_hour_table %>% select(-Traps)))) %>% 
  kable(escape = FALSE, align = c("c", "l", rep("r", 8))) %>% 
  kable_styling("striped", full_width = FALSE) %>% 
  add_header_above(c(" " = 2, "Two-Hour Averages" = 4, "Daily Averages" = 4)) %>% 
  column_spec(2, border_right = TRUE) %>% 
  column_spec(6, border_right = TRUE) %>% 
  collapse_rows(columns = 1) %>%
  pack_rows(start_row = 1, end_row = 3, indent =  FALSE) %>% 
  pack_rows(start_row = 4, end_row = 6, indent =  FALSE) %>% 
  pack_rows(start_row = 7, end_row = 9, indent =  FALSE) %>% 
  pack_rows(start_row = 10, end_row = 12, indent =  FALSE) %>% 
  pack_rows(start_row = 13, end_row = 15, indent =  FALSE) %>% 
  pack_rows(start_row = 16, end_row = 18, indent =  FALSE)

kable_contr_top  
```

```{r}

two_hour_anova <- map(fit_anova_contr, ~.x %>% extract(c(1))) 
daily_anova <- map(fit_anova_contr, ~.x %>% extract(c(5))) 

two_hour_table_anova <- map(two_hour_anova %>% unlist(recursive = FALSE), ~.x %>% 
  summary %>% 
  unclass() %>% 
  bind_cols() %>% 
  as_tibble %>% 
  bind_cols(name = c("Trap", "Residuals")) %>% 
  select(name, Df, `Sum Sq`, `Pr(>F)`) %>% 
  mutate(`Sum Sq` = round(`Sum Sq`),
         `Pr(>F)` = scales::percent(signif(`Pr(>F)`, 3), accuracy = 0.01),
         lead_sq = lead(`Sum Sq`, 1),
         PES = scales::percent(signif(`Sum Sq` / (`Sum Sq` + lead_sq), 3), accuracy = 0.01))) %>% 
  bind_rows

daily_table_anova <- map(daily_anova %>% unlist(recursive = FALSE), ~.x %>% 
  summary %>% 
  unclass() %>% 
  bind_cols() %>% 
  as_tibble %>% 
  bind_cols(name = c("Trap", "Residuals")) %>% 
  select(name, Df, `Sum Sq`, `Pr(>F)`) %>% 
  mutate(`Sum Sq` = round(`Sum Sq`),
         `Pr(>F)` = scales::percent(signif(`Pr(>F)`, 3), accuracy = 0.01),
         lead_sq = lead(`Sum Sq`, 1),
         PES = scales::percent(signif(`Sum Sq` / (`Sum Sq` + lead_sq), 3), accuracy = 0.01))) %>% 
  bind_rows

options(knitr.kable.NA = '')

kable_anova_top <- two_hour_table_anova %>% 
  bind_cols(daily_table_anova %>% setNames(paste0(names(daily_table_anova), 1))) %>% 
  mutate(Species = rep(species_contr, each = 2)) %>% 
  select(Species, name, Df, `Sum Sq`, `Pr(>F)`, PES, Df1, `Sum Sq1`, `Pr(>F)1`, PES1) %>% 
  setNames(c("Species", "Factor", "Df", "Sum Sq", "Pr(>F)", "PES", "Df", "Sum Sq", "Pr(>F)", "PES")) %>% 
  kable(escape = FALSE, align = c("c", "l", rep("r", 8))) %>% 
  kable_styling("striped", full_width = FALSE) %>% 
  add_header_above(c(" " = 2, "Two-Hour Averages" = 4, "Daily Averages" = 4)) %>% 
  column_spec(2, border_right = TRUE) %>% 
  column_spec(6, border_right = TRUE) %>% 
  collapse_rows(columns = 1) %>%
  pack_rows(start_row = 1, end_row = 2, indent =  FALSE) %>% 
  pack_rows(start_row = 3, end_row = 4, indent =  FALSE) %>% 
  pack_rows(start_row = 5, end_row = 6, indent =  FALSE) %>% 
  pack_rows(start_row = 7, end_row = 8, indent =  FALSE) %>% 
  pack_rows(start_row = 9, end_row = 10, indent =  FALSE) %>% 
  pack_rows(start_row = 11, end_row = 12, indent =  FALSE)  

kable_anova_top

```


### Top-5 Species - Robust

```{r}
two_hour_contr <- map(npar_contr_res, ~.x %>% extract(c(1))) 
daily_contr <- map(npar_contr_res, ~.x %>% extract(c(5))) 

two_hour_table <- two_hour_contr %>% 
  unlist(recursive = FALSE) %>% 
  map(~ .x %>% as_tibble) %>% 
  bind_rows() %>% 
  mutate_if(is.numeric, round, 3) %>%
  mutate(species = rep(species_contr, each = 3),
         traps = rep(c("4-2", "8-2", "8-4"), times = 6)) %>% 
  select(Traps = traps, Estimator, Lower, Upper, "p-Value" = "p.Value")

daily_table <- daily_contr %>% 
  unlist(recursive = FALSE) %>% 
  map(~ .x %>% as_tibble) %>% 
  bind_rows() %>% 
  mutate_if(is.numeric, round, 3) %>%
  mutate(species = rep(species_contr, each = 3),
         traps = rep(c("4-2", "8-2", "8-4"), times = 6)) %>% 
  select(Estimator, Lower, Upper, "p-Value" = "p.Value")

kable_robust_top <- two_hour_table %>% 
  mutate(`p-Value` = if_else(`p-Value` < 0.05,
                             cell_spec(`p-Value`, color = "red"),
                             cell_spec(`p-Value`))) %>%
  bind_cols(daily_table) %>% 
  mutate(species = rep(species_contr, each = 3),
         species = cell_spec(species, angle = 270, align =  "center")) %>% 
  select(species, everything()) %>% 
  setNames(c("Species", names(two_hour_table), names(two_hour_table %>% select(-Traps)))) %>% 
  kable(escape = FALSE, align = c("c", "l", rep("r", 8))) %>% 
  kable_styling("striped", full_width = FALSE) %>% 
  add_header_above(c(" " = 2, "Two-Hour Averages" = 4, "Daily Averages" = 4)) %>% 
  column_spec(2, border_right = TRUE) %>% 
  column_spec(6, border_right = TRUE) %>% 
  collapse_rows(columns = 1) %>%
  pack_rows(start_row = 1, end_row = 3, indent =  FALSE) %>% 
  pack_rows(start_row = 4, end_row = 6, indent =  FALSE) %>% 
  pack_rows(start_row = 7, end_row = 9, indent =  FALSE) %>% 
  pack_rows(start_row = 10, end_row = 12, indent =  FALSE) %>% 
  pack_rows(start_row = 13, end_row = 15, indent =  FALSE) %>% 
  pack_rows(start_row = 16, end_row = 18, indent =  FALSE)

kable_robust_top
```

```{r}
two_hour_contr <- map(npar_contr_overall, ~.x %>% extract(c(1)))
daily_contr <- map(npar_contr_overall, ~.x %>% extract(c(5)))

two_hour_table <- two_hour_contr %>%
  unlist(recursive = FALSE) %>%
  bind_rows() %>%
  mutate_if(is.numeric, round, 3) %>%
  mutate(Species = species_contr)

daily_table <- daily_contr %>%
  unlist(recursive = FALSE) %>%
  bind_rows() %>%
  mutate_if(is.numeric, round, 3) %>%
  mutate(Species = species_contr)

two_hour_table %>%
  bind_cols(daily_table %>% setNames(paste0(names(daily_table), 1))) %>%
  select(Species, Quantile, p.Value, Quantile1, p.Value1) %>%
  setNames(c("Species", "Quantile", "p.Value", "Quantile", "p.Value")) %>%
  kable(escape = FALSE, align = c("c",  rep("r", 8))) %>%
  kable_styling("striped", full_width = FALSE) %>%
  add_header_above(c(" " = 1, "Two-Hour Averages" = 2, "Daily Averages" = 2)) %>%
  column_spec(1, border_right = TRUE) %>%
  column_spec(3, border_right = TRUE) %>%
  collapse_rows(columns = 1)

```


### Pollen Size Group

```{r}

fit_anova_contr <- list()
hsd_contr <- list()
hsd_per_res <- list()
npar_contr <- list()
npar_contr_res <- list()
npar_contr_overall <- list()

# Group1 <- c("Urtica", "Castanea") 
# Group2 <- c("Alnus", "Betula", "Corylus", "Fraxinus", "Platanus", "Rumex", "Salix")
# Group3 <- c("Plantago", "Poaceae", "Populus", "Taxus", "Cupressus", "Ulmus")
# Group4 <- c("Carpinus", "Fagus", "Juglans", "Quercus")
# Group5 <- c("Pinus", "Picea")

species_contr <- c("Total", "Group1", "Group2", "Group3", "Group4", "Group5")

# Parametric

for (i in species_contr){
  fit_anova_contr[[i]] <- map(data_contr_tf, ~aov(as.formula(paste(i, "~ trap")), data = .x, contrasts = c("contr.sum", "contr.poly")))  
  hsd_contr[[i]] <- map(fit_anova_contr[[i]], ~TukeyHSD(.x))
  hsd_per_res[[i]] <- map(hsd_contr[[i]], ~.x[["trap"]])
  
  npar_contr[[i]] <- map(data_contr_tf, ~nparcomp::nparcomp(as.formula(paste(i, "~ trap")), data = .x, conf.level = 0.95, alternative = "two.sided", type = "Tukey"))
  npar_contr_res[[i]] <- map(npar_contr[[i]], ~.x[["Analysis"]])
  npar_contr_overall[[i]] <- map(npar_contr[[i]], ~.x[["Overall"]])
}

number_obs_contr <- map(data_contr_tf, ~map(.x, ~length(.x[!is.na(.x)]) / 3))

species_contr_sel <- "Group1" # Specify Resolution Here
title_contr <- paste(species_contr_sel, "Pollen Concentrations")
header_contr <- c(title_contr = 5)
names(header_contr) <- title_contr

hsd_per_res %>% 
  extract2(species_contr_sel) %>% 
  map( ~ .x %>% as_tibble) %>% 
  bind_rows() %>% 
  mutate_all(~round(., 3)) %>% 
  mutate(res = rep(names(data_aggs), each = 3),
         traps = rep(c("4-2", "8-2", "8-4"), times = 5)) %>% 
  select(Traps = traps, "Mean-Difference" = diff, "Lower" = lwr, "Upper" = upr, "p-Value" = "p adj") %>% 
  kable() %>% 
  kable_styling("striped", full_width = FALSE) %>% 
    pack_rows(start_row = 1, end_row = 3, group_label = paste("2-Hour Averages, Observations:", number_obs_contr$data_hours2[[species_contr_sel]])) %>% 
    pack_rows(start_row = 4, end_row = 6, group_label = paste("3-Hour Averages, Observations:", number_obs_contr$data_hours3[[species_contr_sel]])) %>% 
    pack_rows(start_row = 7, end_row = 9, group_label = paste("6-Hour Averages, Observations:", number_obs_contr$data_hours6[[species_contr_sel]])) %>% 
    pack_rows(start_row = 10, end_row = 12, group_label = paste("12-Hour Averages, Observations:", number_obs_contr$data_hours12[[species_contr_sel]])) %>% 
    pack_rows(start_row = 13, end_row = 15, group_label = paste("Daily Averages, Observations:", number_obs_contr$data_daily[[species_contr_sel]])) %>% 
  add_header_above(header_contr)


npar_contr_res %>%
  extract2(species_contr_sel) %>% # Specify Resolution Here
  bind_rows() %>%
  mutate_if(is.numeric, round, 3) %>%
  mutate(res = rep(names(data_aggs), each = 3)) %>%
  select(Traps = Comparison, Estimator, Lower, Upper, "p-Value" = "p.Value") %>%
  kable() %>%
  kable_styling("striped", full_width = FALSE) %>% 
    pack_rows(start_row = 1, end_row = 3, group_label = paste("2-Hour Averages, Observations:", number_obs_contr$data_hours2[[species_contr_sel]])) %>% 
    pack_rows(start_row = 4, end_row = 6, group_label = paste("3-Hour Averages, Observations:", number_obs_contr$data_hours3[[species_contr_sel]])) %>% 
    pack_rows(start_row = 7, end_row = 9, group_label = paste("6-Hour Averages, Observations:", number_obs_contr$data_hours6[[species_contr_sel]])) %>% 
    pack_rows(start_row = 10, end_row = 12, group_label = paste("12-Hour Averages, Observations:", number_obs_contr$data_hours12[[species_contr_sel]])) %>% 
    pack_rows(start_row = 13, end_row = 15, group_label = paste("Daily Averages, Observations:", number_obs_contr$data_daily[[species_contr_sel]])) %>% 
  add_header_above(header_contr)

```

### Pollen Size Group - Anova

```{r}
two_hour_contr <- map(hsd_per_res, ~.x %>% extract(c(1))) 
daily_contr <- map(hsd_per_res, ~.x %>% extract(c(5))) 

two_hour_table <- two_hour_contr %>% 
  unlist(recursive = FALSE) %>% 
  map(~ .x %>% as_tibble) %>% 
  bind_rows() %>% 
  mutate_if(is.numeric, round, 3) %>%
  mutate(traps = rep(c("4-2", "8-2", "8-4"), times = 6)) %>% 
  select(Traps = traps, "Mean-Difference" = diff, "Lower" = lwr, "Upper" = upr, "p-Value" = "p adj")

daily_table <- daily_contr %>% 
  unlist(recursive = FALSE) %>% 
  map(~ .x %>% as_tibble) %>% 
  bind_rows() %>% 
  mutate_if(is.numeric, round, 3) %>%
  mutate(traps = rep(c("4-2", "8-2", "8-4"), times = 6)) %>% 
  select("Mean-Difference" = diff, "Lower" = lwr, "Upper" = upr, "p-Value" = "p adj")

kable_contr_size <- two_hour_table %>% 
  mutate(`p-Value` = if_else(`p-Value` < 0.05, 
                             cell_spec(`p-Value`, color = "red"),
                             cell_spec(`p-Value`))) %>% 
  bind_cols(daily_table) %>% 
  mutate(species = rep(species_contr, each = 3),
         species = cell_spec(species, angle = 270, align =  "center")) %>% 
  select(species, everything()) %>% 
  setNames(c("Species", names(two_hour_table), names(two_hour_table %>% select(-Traps)))) %>% 
  kable(escape = FALSE, align = c("c", "l", rep("r", 8))) %>% 
  kable_styling("striped", full_width = FALSE) %>% 
  add_header_above(c(" " = 2, "Two-Hour Averages" = 4, "Daily Averages" = 4)) %>% 
  column_spec(2, border_right = TRUE) %>% 
  column_spec(6, border_right = TRUE) %>% 
  collapse_rows(columns = 1) %>%
  pack_rows(start_row = 1, end_row = 3, indent =  FALSE) %>% 
  pack_rows(start_row = 4, end_row = 6, indent =  FALSE) %>% 
  pack_rows(start_row = 7, end_row = 9, indent =  FALSE) %>% 
  pack_rows(start_row = 10, end_row = 12, indent =  FALSE) %>% 
  pack_rows(start_row = 13, end_row = 15, indent =  FALSE) %>% 
  pack_rows(start_row = 16, end_row = 18, indent =  FALSE)

kable_contr_size
```


```{r}

two_hour_anova <- map(fit_anova_contr, ~.x %>% extract(c(1))) 
daily_anova <- map(fit_anova_contr, ~.x %>% extract(c(5))) 

two_hour_table_anova <- map(two_hour_anova %>% unlist(recursive = FALSE), ~.x %>% 
  summary %>% 
  unclass() %>% 
  bind_cols() %>% 
  as_tibble %>% 
  bind_cols(name = c("Trap", "Residuals")) %>% 
  select(name, Df, `Sum Sq`, `Pr(>F)`) %>% 
  mutate(`Sum Sq` = round(`Sum Sq`),
         `Pr(>F)` = scales::percent(signif(`Pr(>F)`, 3), accuracy = 0.01),
         lead_sq = lead(`Sum Sq`, 1),
         PES = scales::percent(signif(`Sum Sq` / (`Sum Sq` + lead_sq), 3), accuracy = 0.01))) %>% 
  bind_rows


daily_table_anova <- map(daily_anova %>% unlist(recursive = FALSE), ~.x %>% 
  summary %>% 
  unclass() %>% 
  bind_cols() %>% 
  as_tibble %>% 
  bind_cols(name = c("Trap", "Residuals")) %>% 
  select(name, Df, `Sum Sq`, `Pr(>F)`) %>% 
  mutate(`Sum Sq` = round(`Sum Sq`),
         `Pr(>F)` = scales::percent(signif(`Pr(>F)`, 3), accuracy = 0.01),
         lead_sq = lead(`Sum Sq`, 1),
         PES = scales::percent(signif(`Sum Sq` / (`Sum Sq` + lead_sq), 3), accuracy = 0.01))) %>% 
  bind_rows

options(knitr.kable.NA = '')

kable_anova_size <- two_hour_table_anova %>% 
  bind_cols(daily_table_anova %>% setNames(paste0(names(daily_table_anova), 1))) %>%
  mutate(Species = rep(species_contr, each = 2)) %>% 
  select(Species, name, Df, `Sum Sq`, `Pr(>F)`, PES, Df1, `Sum Sq1`, `Pr(>F)1`, PES1) %>% 
  setNames(c("Species", "Factor", "Df", "Sum Sq", "Pr(>F)", "PES", "Df", "Sum Sq", "Pr(>F)", "PES")) %>% 
  kable(escape = FALSE, align = c("c", "l", rep("r", 8))) %>% 
  kable_styling("striped", full_width = FALSE) %>% 
  add_header_above(c(" " = 2, "Two-Hour Averages" = 4, "Daily Averages" = 4)) %>% 
  column_spec(2, border_right = TRUE) %>% 
  column_spec(6, border_right = TRUE) %>% 
  collapse_rows(columns = 1) %>%
  pack_rows(start_row = 1, end_row = 2, indent =  FALSE) %>% 
  pack_rows(start_row = 3, end_row = 4, indent =  FALSE) %>% 
  pack_rows(start_row = 5, end_row = 6, indent =  FALSE) %>% 
  pack_rows(start_row = 7, end_row = 8, indent =  FALSE) %>% 
  pack_rows(start_row = 9, end_row = 10, indent =  FALSE) %>% 
  pack_rows(start_row = 11, end_row = 12, indent =  FALSE)  

kable_anova_size
```

### Pollen Size Group - Robust

```{r}
two_hour_contr <- map(npar_contr_res, ~.x %>% extract(c(1))) 
daily_contr <- map(npar_contr_res, ~.x %>% extract(c(5))) 

two_hour_table <- two_hour_contr %>% 
  unlist(recursive = FALSE) %>% 
  map(~ .x %>% as_tibble) %>% 
  bind_rows() %>% 
  mutate_if(is.numeric, round, 3) %>%
  mutate(species = rep(species_contr, each = 3),
         traps = rep(c("4-2", "8-2", "8-4"), times = 6)) %>% 
  select(Traps = traps, Estimator, Lower, Upper, "p-Value" = "p.Value")

daily_table <- daily_contr %>% 
  unlist(recursive = FALSE) %>% 
  map(~ .x %>% as_tibble) %>% 
  bind_rows() %>% 
  mutate_if(is.numeric, round, 3) %>%
  mutate(species = rep(species_contr, each = 3),
         traps = rep(c("4-2", "8-2", "8-4"), times = 6)) %>% 
  select(Estimator, Lower, Upper, "p-Value" = "p.Value")

kable_robust_size <- two_hour_table %>% 
  mutate(`p-Value` = if_else(`p-Value` < 0.05,
                             cell_spec(`p-Value`, color = "red"),
                             cell_spec(`p-Value`))) %>%
  bind_cols(daily_table %>% setNames(paste0(names(daily_table), 1))) %>%
  mutate(species = rep(species_contr, each = 3),
         species = cell_spec(species, angle = 270, align =  "center")) %>% 
  select(species, everything()) %>% 
  setNames(c("Species", names(two_hour_table), names(two_hour_table %>% select(-Traps)))) %>% 
  kable(escape = FALSE, align = c("c", "l", rep("r", 8))) %>% 
  kable_styling("striped", full_width = FALSE) %>% 
  add_header_above(c(" " = 2, "Two-Hour Averages" = 4, "Daily Averages" = 4)) %>% 
  column_spec(2, border_right = TRUE) %>% 
  column_spec(6, border_right = TRUE) %>% 
  collapse_rows(columns = 1) %>%
  pack_rows(start_row = 1, end_row = 3, indent =  FALSE) %>% 
  pack_rows(start_row = 4, end_row = 6, indent =  FALSE) %>% 
  pack_rows(start_row = 7, end_row = 9, indent =  FALSE) %>% 
  pack_rows(start_row = 10, end_row = 12, indent =  FALSE) %>% 
  pack_rows(start_row = 13, end_row = 15, indent =  FALSE) %>% 
  pack_rows(start_row = 16, end_row = 18, indent =  FALSE)

kable_robust_size
```


### Single Concentration Group

```{r}

fit_anova_contr <- list()
hsd_contr <- list()
hsd_per_res <- list()
npar_contr <- list()
npar_contr_res <- list()

data_contr_conc <- map(data_contr_tf, ~.x %>% 
  select(-Total:-Group5) %>% 
  pivot_longer(Castanea:Picea) %>% 
  pivot_wider(names_from = trap) %>% 
  setNames(c("timestamp", "type", "trap2", "trap4", "trap8")) %>% 
  mutate(mean = (trap2 + trap4 + trap8)/3) %>% 
  filter(!is.na(mean)) %>% 
  pivot_longer(trap2:trap8, names_to = "trap") %>% 
  mutate(conc = case_when(
    mean >= log(0) & mean < log(10) ~ "Group0_10",
    mean >= log(10) & mean < log(20) ~ "Group10_20",
    mean >= log(20) & mean < log(50) ~ "Group20_50",
    mean >= log(50) & mean < log(100) ~ "Group50_100",
    mean >= log(100) & mean < log(300) ~ "Group100_300",
    mean >= log(300) ~ "Group300"
  )) %>% 
  select(timestamp, trap, value, conc, type) %>% 
  pivot_wider(names_from = conc, values_from = value))


species_contr <- c("Group0_10", "Group10_20", "Group20_50", "Group50_100", "Group100_300", "Group300")

# Parametric

for (i in species_contr){
  fit_anova_contr[[i]] <- map(data_contr_conc, ~aov(as.formula(paste(i, "~ trap")), data = .x, contrasts = c("contr.sum", "contr.poly")))  
  hsd_contr[[i]] <- map(fit_anova_contr[[i]], ~TukeyHSD(.x))
  hsd_per_res[[i]] <- map(hsd_contr[[i]], ~.x[["trap"]])
  # 
  # npar_contr[[i]] <- map(data_contr_conc, ~nparcomp::nparcomp(as.formula(paste(i, "~ trap")), data = .x  %>% filter(!is.na(!!sym(i))), conf.level = 0.95, alternative = "two.sided", type = "Tukey"))  
  # npar_contr_res[[i]] <- map(npar_contr[[i]], ~.x[["Analysis"]])
}

# There is an unsolved error in the nparcomp source-code, this prevents us from running the robust methods on the longer conc dataset: https://stackoverflow.com/questions/24047659/dataset-limitation-in-r-package-nparcomp

number_obs_contr <- map(data_contr_conc, ~map(.x, ~length(.x[!is.na(.x)]) / 3))
min_group <- 0
max_group <- 10
conc_group <- paste0("Group", min_group, "_", max_group)
title_contr <- paste0("Concentration Group from ", min_group, " (", round(log(min_group), 2), ") to ", max_group, " (", round(log(max_group), 2), ")")
header_contr <- c(title_contr = 5)
names(header_contr) <- title_contr

hsd_per_res %>% 
  extract2(conc_group) %>% # Specify Resolution Here
  map( ~ .x %>% as_tibble) %>% 
  bind_rows() %>% 
  mutate_all(~round(., 3)) %>% 
  mutate(res = rep(names(data_aggs), each = 3),
         traps = rep(c("4-2", "8-2", "8-4"), times = 5)) %>% 
  select(Traps = traps, "Mean-Difference" = diff, "Lower" = lwr, "Upper" = upr, "p-Value" = "p adj") %>% 
  kable() %>% 
  kable_styling("striped", full_width = FALSE) %>% 
    pack_rows(start_row = 1, end_row = 3, group_label = paste("2-Hour Averages, Observations:", number_obs_contr$data_hours2[[conc_group]])) %>% 
    pack_rows(start_row = 4, end_row = 6, group_label = paste("3-Hour Averages, Observations:", number_obs_contr$data_hours3[[conc_group]])) %>% 
    pack_rows(start_row = 7, end_row = 9, group_label = paste("6-Hour Averages, Observations:", number_obs_contr$data_hours6[[conc_group]])) %>% 
    pack_rows(start_row = 10, end_row = 12, group_label = paste("12-Hour Averages, Observations:", number_obs_contr$data_hours12[[conc_group]])) %>% 
    pack_rows(start_row = 13, end_row = 15, group_label = paste("Daily Averages, Observations:", number_obs_contr$data_daily[[conc_group]])) %>% 
  add_header_above(header = header_contr)


# npar_contr_res %>% 
#   extract2("Total") %>% # Specify Resolution Here
#   bind_rows() %>% 
#   mutate_if(is.numeric, round, 3) %>% 
#   mutate(res = rep(names(data_aggs), each = 3)) %>% 
#   select(Traps = Comparison, Estimator, Lower, Upper, "p-Value" = "p.Value") %>% 
#   kable() %>% 
#   kable_styling("striped", full_width = FALSE) %>% 
#     pack_rows(start_row = 1, end_row = 3, group_label = "2-Hour Averages") %>% 
#     pack_rows(start_row = 4, end_row = 6, group_label = "3-Hour Averages") %>% 
#     pack_rows(start_row = 7, end_row = 9, group_label = "6-Hour Averages") %>% 
#     pack_rows(start_row = 10, end_row = 12, group_label = "12-Hour Averages") %>% 
#     pack_rows(start_row = 13, end_row = 15, group_label = "Daily Averages")

```

### Concentration Groups - Anova

```{r}
two_hour_contr <- map(hsd_per_res, ~.x %>% extract(c(1)))
daily_contr <- map(hsd_per_res, ~.x %>% extract(c(5)))

two_hour_table <- two_hour_contr %>% 
  unlist(recursive = FALSE) %>% 
  map(~ .x %>% as_tibble) %>% 
  bind_rows() %>% 
  mutate_all(~round(., 3)) %>% 
  mutate(species = rep(species_contr, each = 3),
         traps = rep(c("4-2", "8-2", "8-4"), times = 6)) %>% 
  select(Traps = traps, "Mean-Difference" = diff, "Lower" = lwr, "Upper" = upr, "p-Value" = "p adj")

daily_table <- daily_contr %>% 
  unlist(recursive = FALSE) %>% 
  map(~ .x %>% as_tibble) %>% 
  bind_rows() %>% 
  mutate_all(~round(., 3)) %>% 
  mutate(species = rep(species_contr, each = 3),
         traps = rep(c("4-2", "8-2", "8-4"), times = 6)) %>% 
  select("Mean-Difference" = diff, "Lower" = lwr, "Upper" = upr, "p-Value" = "p adj")

kable_contr_conc <- two_hour_table %>% 
  bind_cols(daily_table %>% setNames(paste0(names(daily_table), 1))) %>%
  mutate(species = rep(species_contr, each = 3),
         species = cell_spec(species, angle = 270, align =  "center")) %>% 
  select(species, everything()) %>% 
  mutate(`p-Value` = if_else(`p-Value` < 0.05, 
                             cell_spec(`p-Value`, color = "red"),
                             cell_spec(`p-Value`)),
         `p-Value1` = if_else(`p-Value1` < 0.05, 
                             cell_spec(`p-Value1`, color = "red"),
                             cell_spec(`p-Value1`))) %>% 
  setNames(c("Species", names(two_hour_table), names(two_hour_table %>% select(-Traps)))) %>% 
  kable(escape = FALSE, align = c("c", "l", rep("r", 8))) %>% 
  kable_styling("striped", full_width = FALSE) %>% 
  add_header_above(c(" " = 2, "Two-Hour Averages" = 4, "Daily Averages" = 4)) %>% 
  column_spec(2, border_right = TRUE) %>% 
  column_spec(6, border_right = TRUE) %>% 
  collapse_rows(columns = 1) %>%
  pack_rows(start_row = 1, end_row = 3, indent =  FALSE) %>% 
  pack_rows(start_row = 4, end_row = 6, indent =  FALSE) %>% 
  pack_rows(start_row = 7, end_row = 9, indent =  FALSE) %>% 
  pack_rows(start_row = 10, end_row = 12, indent =  FALSE) %>% 
  pack_rows(start_row = 13, end_row = 15, indent =  FALSE) %>% 
  pack_rows(start_row = 16, end_row = 18, indent =  FALSE)

kable_contr_conc  
```

```{r}

two_hour_anova <- map(fit_anova_contr, ~.x %>% extract(c(1))) 
daily_anova <- map(fit_anova_contr, ~.x %>% extract(c(5))) 

two_hour_table_anova <- map(two_hour_anova %>% unlist(recursive = FALSE), ~.x %>% 
  summary %>% 
  unclass() %>% 
  bind_cols() %>% 
  as_tibble %>% 
  bind_cols(name = c("Trap", "Residuals")) %>% 
  select(name, Df, `Sum Sq`, `Pr(>F)`) %>% 
  mutate(`Sum Sq` = round(`Sum Sq`),
         `Pr(>F)` = scales::percent(signif(`Pr(>F)`, 3), accuracy = 0.01),
         lead_sq = lead(`Sum Sq`, 1),
         PES = scales::percent(signif(`Sum Sq` / (`Sum Sq` + lead_sq), 3), accuracy = 0.01))) %>% 
  bind_rows


daily_table_anova <- map(daily_anova %>% unlist(recursive = FALSE), ~.x %>% 
  summary %>% 
  unclass() %>% 
  bind_cols() %>% 
  as_tibble %>% 
  bind_cols(name = c("Trap", "Residuals")) %>% 
  select(name, Df, `Sum Sq`, `Pr(>F)`) %>% 
  mutate(`Sum Sq` = round(`Sum Sq`),
         `Pr(>F)` = scales::percent(signif(`Pr(>F)`, 3), accuracy = 0.01),
         lead_sq = lead(`Sum Sq`, 1),
         PES = scales::percent(signif(`Sum Sq` / (`Sum Sq` + lead_sq), 3), accuracy = 0.01))) %>% 
  bind_rows

options(knitr.kable.NA = '')

kable_anova_conc <- two_hour_table_anova %>% 
  bind_cols(daily_table_anova %>% setNames(paste0(names(daily_table_anova), 1))) %>%
  mutate(Species = rep(species_contr, each = 2)) %>% 
  select(Species, name, Df, `Sum Sq`, `Pr(>F)`, PES, Df1, `Sum Sq1`, `Pr(>F)1`, PES1) %>% 
  setNames(c("Species", "Factor", "Df", "Sum Sq", "Pr(>F)", "PES", "Df", "Sum Sq", "Pr(>F)", "PES")) %>% 
  kable(escape = FALSE, align = c("c", "l", rep("r", 8))) %>% 
  kable_styling("striped", full_width = FALSE) %>% 
  add_header_above(c(" " = 2, "Two-Hour Averages" = 4, "Daily Averages" = 4)) %>% 
  column_spec(2, border_right = TRUE) %>% 
  column_spec(6, border_right = TRUE) %>% 
  collapse_rows(columns = 1) %>%
  pack_rows(start_row = 1, end_row = 2, indent =  FALSE) %>% 
  pack_rows(start_row = 3, end_row = 4, indent =  FALSE) %>% 
  pack_rows(start_row = 5, end_row = 6, indent =  FALSE) %>% 
  pack_rows(start_row = 7, end_row = 8, indent =  FALSE) %>% 
  pack_rows(start_row = 9, end_row = 10, indent =  FALSE) %>% 
  pack_rows(start_row = 11, end_row = 12, indent =  FALSE)  

kable_anova_conc
```

## Exports

```{r eval = FALSE}

# For rendering .docx tables change the file ending from .png to .doc
# webshot::install_phantomjs() # needed for saveing kables and improved results with magick package

kableExtra::save_kable(kable_metrics, file = "table1_metrics_spi_allspecies.doc")
kableExtra::save_kable(kable_number_traps, file = "table2_number_traps.doc")
kableExtra::save_kable(kable_cv_top, file = "table3_cv_top_species.doc")
kableExtra::save_kable(kable_cv_size, file = "table4_cv_size_groups.doc")
kableExtra::save_kable(kable_cv_conc, file = "table5_cv_conc_groups.doc")
kableExtra::save_kable(kable_anova_top, file = "table6_anova_top_species.doc")
kableExtra::save_kable(kable_anova_size, file = "table7_anova_size_groups.doc")
kableExtra::save_kable(kable_anova_conc, file = "table8_anova_conc_groups.doc")
kableExtra::save_kable(kable_contr_top, file = "table9_anova_contrast_top_species.doc")
kableExtra::save_kable(kable_contr_size, file = "table10_anova_contrast_size_groups.doc")
kableExtra::save_kable(kable_contr_conc, file = "table11_anova_contrast_conc_groups.doc")
kableExtra::save_kable(kable_robust_top, file = "table12_robust_contrast_top_species.doc")
kableExtra::save_kable(kable_robust_size, file = "table13_robust_contrast_size_groups.doc")

# The GG-Plots are being saved right here:

#### PER SPECIES AND TEMPORAL AVERAGE
ggsave("figure1_timeseries_boxplot_hist.png", gg_comparison, width = 10, height = 7, dpi = 300)
ggsave("figure2_timeseries.png", gg_timeseries, width = 10, height = 7, dpi = 300)
ggsave("figure3_correlation.png", gg_corr, width = 10, height = 7, dpi = 300)
ggsave("figure4_altman_bland.png", gg_ab, width = 10, height = 7, dpi = 300)
ggsave("figure9_boxplot_reldiff.png", gg_boxplot_reldiff, width = 10, height = 7, dpi = 300)

#### PER TEMPORAL AVERAGE
ggsave("figure5_error_hist_species.png", gg_error_species, width = 10, height = 7, dpi = 300)
ggsave("figure6_error_hist_size.png", gg_error_size, width = 10, height = 7, dpi = 300)
ggsave("figure7_error_hist_conc.png", gg_error_conc, width = 10, height = 7, dpi = 300)
ggsave("figure8_measurement_differences_conc_groups.png", gg_dens_conc, width = 10, height = 7, dpi = 300)
```



